{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_4MJgf8Ftg2"
      },
      "source": [
        "# Loan default prediction\n",
        "\n",
        "The problem is defined in the classification framework, where the predicted variable\n",
        "is “Charge-Off ”. A charge-off is a debt that a creditor has given up trying to collect on\n",
        "after you’ve missed payments for several months. The predicted variable takes value 1\n",
        "in case of charge-off and 0 otherwise.\n",
        "\n",
        "This case study aims to analyze data for loans through 2007-2017Q3 from Lending Club available on Kaggle. Dataset contains over 887 thousand observations and 150 variables among which one is describing the loan status."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svdQMjimgLeY"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    # Try importing the SHAP library\n",
        "    import shap\n",
        "    print(\"SHAP is already installed.\")\n",
        "\n",
        "except ImportError:\n",
        "    # If SHAP is not installed, install it using pip\n",
        "    print(\"SHAP is not installed. Installing SHAP...\")\n",
        "    !pip install shap\n",
        "    print(\"SHAP installed successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C91MNoMpFtg3"
      },
      "outputs": [],
      "source": [
        "# Importing required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import shap\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, log_loss, accuracy_score, precision_score, recall_score, f1_score, roc_curve, roc_auc_score\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.inspection import partial_dependence, PartialDependenceDisplay\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3dLmGpvgLeZ"
      },
      "outputs": [],
      "source": [
        "def evaluate_classification_model(model, X_test, y_test, class_labels=['Not Charged Off', 'Charged Off']):\n",
        "    \"\"\"\n",
        "    Evaluate a classification model and display various metrics.\n",
        "\n",
        "    Parameters:\n",
        "    - model: The trained machine learning model\n",
        "    - X_test: Test set features\n",
        "    - y_test: True labels for the test set\n",
        "    - class_labels: List of labels for the classification task\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    # Predictions for the test set\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    # Classification Report\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred, target_names=class_labels))\n",
        "\n",
        "    # Log Loss\n",
        "    log_loss_score = log_loss(y_test, y_pred_proba)\n",
        "    print(f\"Log Loss: {log_loss_score:.4f}\")\n",
        "\n",
        "    # F1 Score\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "    # Confusion Matrix\n",
        "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
        "    plt.xlabel('Predicted labels')\n",
        "    plt.ylabel('True labels')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "    # ROC Curve\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
        "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('ROC Curve')\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Area Under ROC (AUC): {roc_auc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_Z2j6A6Ftg4"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load the dataset into a Pandas DataFrame\n",
        "file_path = './13_german_credit_data.csv'\n",
        "loans_df = pd.read_csv(file_path)\n",
        "\n",
        "# Display the first few rows of the DataFrame for initial exploration\n",
        "loans_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wlUXrf8sFtg5"
      },
      "outputs": [],
      "source": [
        "# Check data types\n",
        "data_info = loans_df.info()\n",
        "data_info\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWPszoJ3Ftg5"
      },
      "source": [
        "Identifying Categorical Columns\n",
        "\n",
        "Need for One-Hot Encoding in Decision Trees\n",
        "For many machine learning algorithms, one-hot encoding is necessary for the proper interpretation of categorical variables. For many tree based algorithms this is not the case( especcialy cat-boost).\n",
        "\n",
        "However, the specific implementation in scikit-learn for models like DecisionTreeClassifier, RandomForestClassifier, and GradientBoostingClassifier does not natively support categorical variables.\n",
        "\n",
        "We will then need to onehotencode our data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8MngGADgLea"
      },
      "outputs": [],
      "source": [
        "# Detect categorical columns\n",
        "categorical_cols = loans_df.select_dtypes(include=['object', 'category']).columns\n",
        "\n",
        "# Initialize encoder\n",
        "encoder = OneHotEncoder(\n",
        "    drop='first',\n",
        "    sparse_output=False,\n",
        "    dtype=int,\n",
        "    handle_unknown='ignore'\n",
        ").set_output(transform='pandas')\n",
        "\n",
        "encoded_df = (\n",
        "    encoder\n",
        "    .fit_transform(loans_df[categorical_cols])\n",
        "    .astype(\"Int64\")\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQtujACRgLea"
      },
      "outputs": [],
      "source": [
        "encoded_loans_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Wa1vWHXgLea"
      },
      "source": [
        "Let's seperate X and Y and split our data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRgrGEbmFtg5"
      },
      "outputs": [],
      "source": [
        "# Features (X) and Target (y) variables\n",
        "X = encoded_loans_df.drop('CreditRisk', axis=1)\n",
        "y = loans_df['CreditRisk'] -1\n",
        "\n",
        "# Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Display the shape of the resulting sets\n",
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWA2ZQliFtg5"
      },
      "source": [
        "The DecisionTreeClassifier in scikit-learn offers several hyperparameters that control various aspects of the tree. Some notable ones are:\n",
        "\n",
        "* criterion: The function to measure the quality of a split. Supported criteria are \"gini\" for Gini impurity and \"entropy\" for information gain.\n",
        "* splitter: The strategy used to choose the split at each node. Options are \"best\" to choose the best split and \"random\" to choose the best random split.\n",
        "* max_depth: The maximum depth of the tree.\n",
        "* min_samples_split: The minimum number of samples required to split an internal node.\n",
        "* min_samples_leaf: The minimum number of samples required to be at a leaf node.\n",
        "\n",
        "Applying with Default Parameters\n",
        "\n",
        "Let's first build a decision tree using the default parameters and evaluate its performance using log loss as the metric. Log loss provides a measure of uncertainty and is sensitive to the probabilities assigned to each class, making it a good choice for imbalanced datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dXrM-0c4Ftg6"
      },
      "outputs": [],
      "source": [
        "# Initialize the DecisionTreeClassifier with default parameters\n",
        "\n",
        "# Fit the model to the training data using cross_val_score cv=5 and log loss as the scoring metric\n",
        "cv_results_dt = \n",
        "cv_results_dt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Average 5-Fold CV Log Loss: {}\".format(round(abs(np.mean(cv_results_dt)), 3)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dgBdHI7Ftg6"
      },
      "source": [
        "Ok we have a starting point, but our decision tree requires parameters. Let's apply what we have learned about cross validaiton and hyperparameter tuning. Let's search the following grid or potential paramters:\n",
        "\n",
        "```\n",
        "param_grid = {\n",
        "    'criterion': ['gini', 'entropy'],\n",
        "    'max_depth': [10, 20, 30, 40],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXJoP7hFFtg6"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define hyperparameter grid\n",
        "\n",
        "# Initialize the GridSearchCV object\n",
        "\n",
        "# Fit the model to the training data\n",
        "\n",
        "# Best hyperparameters and score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGUasI-PFtg6"
      },
      "source": [
        "A big improvement. But we have another way to improve our decision trees...\n",
        "\n",
        "**Pruning** is generally applied to combat overfitting. A decision tree that is too deep tends to capture noise and makes the model complex, thereby failing to generalize well to unseen data. The max_depth parameter essentially acts as a pruning mechanism. Another explicit pruning strategy is to use ccp_alpha, a complexity parameter used for Minimal Cost-Complexity Pruning. A tree with a greater ccp_alpha will be more pruned, hence simpler.\n",
        "\n",
        "The Cost-Complexity Pruning parameter (ccp_alpha) penalizes the tree for its complexity. The higher the ccp_alpha, the simpler the tree will be. To demonstrate, let's fit the model with the same hyperparameters as before but with varying ccp_alpha values, and observe the impact on log loss.\n",
        "\n",
        "We could set ccp_alpha using GridSearchCV as we have previosuly seen. Here we ho this manually"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRutPTxXgLeb"
      },
      "outputs": [],
      "source": [
        "# Define hyperparameter grid\n",
        "}\n",
        "\n",
        "# Initialize the GridSearchCV object\n",
        "\n",
        "# Fit the model to the training data\n",
        "\n",
        "# Best hyperparameters and score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wrbPqamFtg7"
      },
      "source": [
        "Pruning helped!\n",
        "\n",
        "Let's fit our final classifier on all our training data and test it's performance on the test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w26U74VsFtg7"
      },
      "outputs": [],
      "source": [
        "# Initialize the DecisionTreeClassifier with the assumed parameters\n",
        "dt_classifier_final = DecisionTreeClassifier(**best_params, random_state=42)\n",
        "\n",
        "# Fit the model to the training data\n",
        "dt_classifier_final.fit(X_train, y_train)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# use the function evaluate_classification_model to see our results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BiIBNHrmFtg7"
      },
      "source": [
        "### Visualizing the Decision Tree\n",
        "\n",
        "Visualizing a decision tree can provide valuable insights into the decision-making process of the model. It allows us to understand the splits and conditions that the tree uses to make predictions.\n",
        "\n",
        "To visualize the decision tree, we can use the plot_tree function from the sklearn.tree module. Due to the potentially large size of the tree, it's often practical to limit the depth of the tree displayed.\n",
        "\n",
        "Let's proceed with the visualization of the Decision Tree model we've just trained, limiting the depth for better readability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gg6gitIdFtg7"
      },
      "outputs": [],
      "source": [
        "# Initialize the plot\n",
        "plt.figure(figsize=(20, 10))\n",
        "\n",
        "# Plot the tree, limiting the depth to 3 levels for better readability\n",
        "plot_tree(dt_classifier_final, filled=True, rounded=True, max_depth=3, feature_names=X_train.columns.to_list(), class_names=['Not Charged Off', 'Charged Off'])\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8rOf7MhFtg7"
      },
      "source": [
        "We don't have a great classifier!\n",
        "\n",
        "Let's explore balanced class weights and f1 as our metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XvjKEp1EFtg7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEp_bcYPFtg8"
      },
      "outputs": [],
      "source": [
        "# Use evaluate_classification_model(grid_search_dtb, X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GcgC5Qu6gLeb"
      },
      "outputs": [],
      "source": [
        "# Initialize the plot\n",
        "plt.figure(figsize=(20, 10))\n",
        "\n",
        "# Plot the tree, limiting the depth to 5 levels for better readability\n",
        "plot_tree(dtb_model, filled=True, rounded=True, max_depth=3, feature_names=X_train.columns.to_list(), class_names=['Not Charged Off', 'Charged Off'])\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgZoVmkZFtg8"
      },
      "source": [
        "### Decision Trees vs Random Forests\n",
        "\n",
        "#### Decision Trees:\n",
        "1. **Simplicity**: A single decision tree is simple to understand and interpret. It makes decisions by splitting the feature space based on feature importance.\n",
        "2. **Prone to Overfitting**: A deep tree tends to fit noise in the data, leading to poor generalization on new, unseen data.\n",
        "3. **Deterministic**: The same set of rules (tree structure) will always be generated for a given dataset, making the model deterministic.\n",
        "\n",
        "#### Random Forests:\n",
        "1. **Ensemble Method**: A Random Forest is an ensemble of decision trees, typically trained via the bagging method.\n",
        "2. **Robust to Overfitting**: By averaging over multiple decision trees, the Random Forest is less likely to overfit to the training data.\n",
        "3. **Stochasticity**: The method introduces randomness in two ways: by bootstrapping samples and by considering a random subset of features for each split.\n",
        "4. **Higher Complexity**: Random Forests are generally more complex and computationally expensive compared to a single decision tree.\n",
        "\n",
        "### Key Parameters in RandomForestClassifier\n",
        "\n",
        "- `n_estimators`: Number of trees in the forest.\n",
        "- `criterion`: The function to measure the quality of a split ('gini' or 'entropy').\n",
        "- `max_depth`: The maximum depth of the tree.\n",
        "- `min_samples_split`: The minimum number of samples required to split an internal node.\n",
        "- `min_samples_leaf`: The minimum number of samples required to be at a leaf node.\n",
        "- `max_features`: The number of features to consider when looking for the best split.\n",
        "- `bootstrap`: Whether bootstrap samples are used when building trees.\n",
        "- `class_weight`: Weights associated with classes, useful for imbalanced datasets.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WveISXiGFtg8"
      },
      "outputs": [],
      "source": [
        "# Define the hyperparameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100],\n",
        "    'criterion': ['gini', 'entropy'],\n",
        "    'max_depth': [5, 10],\n",
        "    'min_samples_split': [2, 0.01],\n",
        "    'class_weight': ['balanced']\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search_rf = GridSearchCV(\n",
        "    RandomForestClassifier(random_state=42),\n",
        "    param_grid,\n",
        "    scoring='f1',\n",
        "    cv=5,\n",
        "    verbose=1,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "grid_search_rf.fit(X_train, y_train)\n",
        "rf_best_param = grid_search_rf.best_params_\n",
        "print(rf_best_param)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fiKKhRmxFtg8"
      },
      "outputs": [],
      "source": [
        "# we can extract the rf model directly from the grid search object\n",
        "rf_classifier_best = grid_search_rf.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iz20oa6nFtg8"
      },
      "outputs": [],
      "source": [
        "# Number of trees in the final forest (Random Forest model: rf_classifier)\n",
        "num_trees = len(rf_classifier_best.estimators_)\n",
        "print(f\"Number of trees in the final forest: {num_trees}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SkftCr05Ftg_"
      },
      "outputs": [],
      "source": [
        "evaluate_classification_model(rf_classifier_best, X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S41NNmj2Ftg_"
      },
      "source": [
        "#### Exploring Feature Importance\n",
        "Random Forests have the advantage of offering a straightforward way to assess feature importance. This is done by looking at the average impurity decrease computed from all decision trees in the forest. Alternatively, one can also look at how much each feature decreases the impurity when it is used for splitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uS-zqvorFtg_"
      },
      "outputs": [],
      "source": [
        "# Extract feature importances\n",
        "feature_importances = pd.DataFrame({\n",
        "    'Feature': X_train.columns,\n",
        "    'Importance': rf_classifier_best.feature_importances_\n",
        "})\n",
        "\n",
        "# Sort by importance\n",
        "feature_importances = feature_importances.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(15, 10))\n",
        "sns.barplot(x='Importance', y='Feature', data=feature_importances)\n",
        "plt.title('Feature Importance')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xA0qt5q5gLec"
      },
      "source": [
        "Partial Dependence Plots:\n",
        "\n",
        "These plots show the marginal effect one or two features have on the predicted outcome. They capture the way a model depends on the features and can be used for both classification and regression models.\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.inspection.PartialDependenceDisplay.html\n",
        "\n",
        "The process of generating a PDP involves the following steps, which are focused entirely on making predictions:\n",
        "\n",
        "* Fixing Feature Values: For a given feature (or a pair of features in the case of interaction effects), a range of values is selected. These values typically span the range observed in the dataset.\n",
        "\n",
        "* Altering Dataset: For each value in this range, the dataset is altered such that the feature(s) of interest are set to this fixed value, while all other features remain unchanged.\n",
        "\n",
        "* Making Predictions: The trained model then makes predictions for each of these altered datasets. This step does not involve any training or modification of the model itself; it simply uses the model to predict outcomes based on the modified input data.\n",
        "\n",
        "* Averaging Predictions: The predictions across these datasets are averaged to obtain a measure of the marginal effect of the feature(s) on the model's prediction. This average is what is plotted in a PDP.\n",
        "\n",
        "Note: This method is a little slow for random forests, because it needs to make lot's of predictions, and to do this random forests needs to use lot's of trees! I have reduced the data to be used with `subsample` and the number of values to be tested for each feature with `grid_resolution`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MA8kGgUZgLec"
      },
      "outputs": [],
      "source": [
        "# Plotting partial dependence for the first feature and interaction between the first and second features\n",
        "PartialDependenceDisplay.from_estimator(rf_classifier_best, X_train, [1,4, (1, 4)])\n",
        "\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Y2ExIQfgLec"
      },
      "source": [
        "### SHAP (SHapley Additive exPlanations)\n",
        "\n",
        "SHAP is a powerful tool for interpreting machine learning models based on game theory. It provides a way to allocate the contribution of each feature for a particular prediction in a consistent and theoretically justified manner. SHAP values offer high-level interpretability without sacrificing accuracy, and they can be used with any machine learning model, though they are most commonly used with tree-based models like Gradient Boosted Trees (GBTs).\n",
        "\n",
        "https://shap.readthedocs.io/en/latest/example_notebooks/overviews/An%20introduction%20to%20explainable%20AI%20with%20Shapley%20values.html\n",
        "\n",
        "### Why Use SHAP?\n",
        "\n",
        "1. **Consistency**: If a model changes such that a feature has a larger impact on the output, the SHAP value for that feature should also change in the same direction.\n",
        "2. **Local Accuracy**: The sum of the SHAP values for a prediction plus the base value should equal the model's output for that instance.\n",
        "3. **Fair Allocation**: The sum of all feature attributions should sum up to the total prediction minus the base value for each instance.\n",
        "\n",
        "### Key SHAP Visualizations:\n",
        "\n",
        "1. **Summary Plot**: Provides an overview of the impact of all features on the model's output.\n",
        "2. **Force Plot**: Visualizes the features pushing a particular prediction higher or lower.\n",
        "3. **Dependence Plot**: Shows how the model output changes with a single feature.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJuL3U-igLed"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Initialize the explainer object with your trained GBT model\n",
        "# Convert X_test to a float type to avoid the TypeError\n",
        "explainer = shap.TreeExplainer(rf_classifier_best, X_test)\n",
        "\n",
        "# Compute SHAP values for the entire test set\n",
        "shap_values = explainer.shap_values(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_m4CMpRlgLed"
      },
      "outputs": [],
      "source": [
        "shap.summary_plot(shap_values, X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KTmP1i2Ftg_"
      },
      "source": [
        "### Gradient Boosted Trees (GBTs) vs Random Forests\n",
        "\n",
        "#### Gradient Boosted Trees:\n",
        "\n",
        "1. **Sequential Learning**: Builds one tree at a time, where each new tree corrects the errors of the previous one.\n",
        "2. **Focus on Error**: Concentrates more on the instances that are hard to classify.\n",
        "3. **Less Randomness**: Unlike Random Forest, there's no bootstrapping of data.\n",
        "4. **Regularization**: Incorporates parameters to regularize the model, reducing the risk of overfitting.\n",
        "5. **Sensitive to Noisy Data**: Since it emphasizes misclassified points, it might give weight to outliers or noise.\n",
        "  \n",
        "#### Random Forests:\n",
        "\n",
        "1. **Parallel Learning**: Builds multiple trees in parallel.\n",
        "2. **Equal Focus**: Treats all instances equally.\n",
        "3. **Bootstrapping**: Utilizes bootstrapping to create different subsets of data.\n",
        "4. **No Regularization**: Less prone to overfitting but does not have built-in regularization.\n",
        "5. **Robust to Noisy Data**: Due to averaging across multiple trees, Random Forests are usually robust to outliers and noise.\n",
        "\n",
        "### Key Parameters in GradientBoostingClassifier\n",
        "\n",
        "- `n_estimators`: Number of boosting stages, i.e., the number of trees.\n",
        "- `learning_rate`: A factor to shrink the contribution of each tree (regularization).\n",
        "- `max_depth`: Maximum depth of the individual regression estimators (trees).\n",
        "- `min_samples_split`: The minimum number of samples required to split an internal node.\n",
        "- `min_samples_leaf`: The minimum number of samples required to be at a leaf node.\n",
        "- `subsample`: Fraction of samples used for fitting the trees.\n",
        "  \n",
        "What Does subsample Do?\n",
        "\n",
        "When subsample=1.0 (default), all training samples are used to build each tree, and no sampling is performed. This is similar to traditional boosting.\n",
        "When 0 < subsample < 1.0, only a fraction of the training samples are used to fit each base learner. For instance, if subsample=0.8, each tree is trained on 80% of the total training samples, chosen randomly.\n",
        "\n",
        "Trade-offs:\n",
        "* Higher Value: More accurate individual trees, but risk of overfitting.\n",
        "* Lower Value: More regularization and faster training, but individual trees may be less accurate.\n",
        "\n",
        "\n",
        "## Exercise:\n",
        "\n",
        "Use grid search and CV to find the parameters for `GradientBoostingClassifier`. A Grid serach object is provided below. Set the scoring to \"f1\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTlDmk5gFtg_"
      },
      "outputs": [],
      "source": [
        "# Define the hyperparameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'max_depth': [3, 5, 10],\n",
        "    'min_samples_split': [2, 0.1]\n",
        "}\n",
        "\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "\n",
        "# The following line would fit the model, but we won't run it here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWD869x9gLed"
      },
      "source": [
        "Use the ´evaluate_classification_model´ to check how the model is performing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RY9pdIWFthA"
      },
      "outputs": [],
      "source": [
        "evaluate_classification_model(gbt_classifier_best, X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdCFThKugLee"
      },
      "source": [
        "Plot feature importances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVw9UwmxFthA"
      },
      "outputs": [],
      "source": [
        "# Extract feature importances\n",
        "feature_importances = pd.DataFrame({\n",
        "    'Feature': X_train.columns,\n",
        "    'Importance': gbt_classifier_best.feature_importances_\n",
        "})\n",
        "\n",
        "# Sort by importance\n",
        "feature_importances = feature_importances.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(15, 10))\n",
        "sns.barplot(x='Importance', y='Feature', data=feature_importances)\n",
        "plt.title('Feature Importance')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLCQ1UthFthA"
      },
      "source": [
        "Plot partial dependence of Credit Duration and Age."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbVEIEETFthA"
      },
      "outputs": [],
      "source": [
        "# Plotting partial dependence for the first feature and interaction between the first and second features\n",
        "PartialDependenceDisplay.from_estimator(gbt_classifier_best, X_train, [0,4, (0, 4)])\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luH5vWnugLee"
      },
      "source": [
        "Create a summary plot whith shap:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9mnMekIgLee"
      },
      "outputs": [],
      "source": [
        "# Initialize the explainer object with your trained GBT model\n",
        "\n",
        "\n",
        "# Compute SHAP values for the entire test set\n",
        "\n",
        "# Create the summary plot\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7VeymDsgLee"
      },
      "source": [
        "Interpretation:\n",
        "* Higher (or Lower) Impact: Features at the top of the plot are typically those that have greater impact on the model's output. The \"impact\" is often computed based on the mean absolute SHAP value for each feature across the dataset.\n",
        "\n",
        "* Direction of Impact: Dots to the right of the zero line on the x-axis typically indicate a positive impact on the prediction, while dots to the left indicate a negative impact.\n",
        "\n",
        "* Magnitude of Impact: The further a dot is from the zero line on the x-axis, the larger the impact that particular feature value had on the prediction for that instance.\n",
        "\n",
        "* Feature Value: The color of the dot represents the value of the feature. This can help you understand how different ranges of feature values are influencing the output.\n",
        "\n",
        "## Exercise\n",
        "\n",
        "See if pruning or stochastic gradient boosting improves the model, check parameters on:\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UrXDv2KRgLee"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCzw4Kg0gLee"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
