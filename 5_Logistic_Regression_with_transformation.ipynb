{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itZSqNkPUX89"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score, auc, mean_squared_error, accuracy_score, precision_score, recall_score, f1_score, ConfusionMatrixDisplay, RocCurveDisplay\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXM3VrJAUX9I"
      },
      "source": [
        "## Let's try to put it all into practice\n",
        "\n",
        "We will use a german credit reisk dataset that classifies people into good or bad risks from:\n",
        "https://archive.ics.uci.edu/dataset/144/statlog+german+credit+data\n",
        "\n",
        "First we will need to load our data (you will also need to upload the data to colab first):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ewREl2W9UX9I"
      },
      "outputs": [],
      "source": [
        "# Load the CSV file into a DataFrame\n",
        "data = pd.read_csv('4_german_credit_data.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IPIW6RDUX9I"
      },
      "source": [
        "After loading the dataset, it's essential to get a quick overview of its characteristics, such as the number of rows and columns, the data types of the columns, and basic statistics of numerical columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7bfkC-zUX9I"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykNVFff6UX9I"
      },
      "source": [
        "What we can see is that the dataset contains 21 columns: 20 features and 1 target column (CreditRisk).\n",
        "Features are a mix of categorical (e.g., CheckingAccountStatus, CreditHistory) and numerical (e.g., Duration, CreditAmount) data types.\n",
        "The target column CreditRisk has two classes: 1 (Good Risk) and 2 (Bad Risk).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixUDeLptUX9J"
      },
      "source": [
        "Let's pick some features:\n",
        "\n",
        "* Duration: As we observed, it has a positive correlation with the target variable.\n",
        "* Amount: The amount of the loan can be an indicator of risk.\n",
        "* Age: Age might play a role in determining creditworthiness.\n",
        "* CheckingAccountStatus: As seen in the plot, different levels of checking account status have different risk distributions.\n",
        "* CreditHistory: The credit history of a person can be a significant indicator of their credit risk.\n",
        "* Purpose: The purpose of the loan can influence the risk associated with it.\n",
        "* SavingsAccountBonds: Savings can be an indicator of financial stability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4GfTY5gUX9J"
      },
      "outputs": [],
      "source": [
        "# Identify numerical and categorical columns\n",
        "numeric_features = ['Duration', 'CreditAmount', 'Age']\n",
        "categorical_features = ['CheckingAccountStatus', 'CreditHistory', 'Purpose', 'SavingsAccountBonds']\n",
        "\n",
        "# Selecting initial set of features\n",
        "selected_features = numeric_features + categorical_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLo5CrK2W2dP"
      },
      "outputs": [],
      "source": [
        "print(selected_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgCBRUZGWm1E"
      },
      "source": [
        "Let's create our data for ML:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_ydvNfjWm1E"
      },
      "outputs": [],
      "source": [
        "X = data[selected_features]\n",
        "y = data['CreditRisk'] - 1  # Convert to 0 and 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1m6OF7aUX9J"
      },
      "source": [
        "We are going to apply one-hot encoding to the categorical features.\n",
        "\n",
        "One-Hot Encoding is a method used to convert categorical data variables so they can be provided to machine learning algorithms to improve predictions. Machine learning algorithms cannot work with categorical data directly. Categorical data must be converted to numbers. One of the most common ways to do this transformation is by using one-hot encoding.\n",
        "\n",
        "How It Works:\n",
        "Identify Unique Categories: For each categorical variable, identify the unique categories it can take.\n",
        "\n",
        "Create New Columns: For each unique category, a new binary (0 or 1) column is created.\n",
        "\n",
        "Binary Indication:\n",
        "\n",
        "For each record, the column corresponding to the category the variable takes value will have a '1', and all other new columns will have a '0'.\n",
        "This means that out of all the new columns for a categorical variable, only one can take the value '1' for a given record.\n",
        "\n",
        "**Important** One drawback of one-hot encoding is that it can increase the dimensionality of the dataset significantly if the categorical variable has many unique values.\n",
        "It can also lead to multicollinearity, a situation where two or more variables are highly correlated. In the context of one-hot encoding, one variable can be predicted from the others. This is called the dummy variable trap.\n",
        "To avoid the dummy variable trap, one common practice is to drop one of the one-hot encoded columns (hence, for a variable with n categories, we keep n−1 dummy columns).\n",
        "\n",
        "This time we will use onehotencoder from scikitlearn and note that logistic regression will have a problem with multicolinearity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vlv5AFYWm1E"
      },
      "source": [
        "`OneHotEncoder` is used to convert categorical data into a binary matrix. It creates one column for each unique category in the data, and for each sample, only the column corresponding to its category will have a value of 1 (all other columns are 0).\n",
        "\n",
        "**Use Case:** OneHotEncoder is commonly used when working with machine learning models that cannot directly handle categorical data.\n",
        "\n",
        "**Documentation:** [OneHotEncoder - scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)\n",
        "\n",
        "**IMPORTANT** Before or after training and test split?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bsbFPrykUX9K"
      },
      "outputs": [],
      "source": [
        "# Preprocess categorical features using fit or fit_transform or fit and transform\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZuhHhYBeXMcQ"
      },
      "outputs": [],
      "source": [
        "# Take a look at the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xu8sHKoeadjx"
      },
      "source": [
        "We will need to get our column names if we want a dataframe later, these will be different remember. We will need to use:\n",
        "```.get_feature_names_out(categorical_features)```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dYXH9tsXUZg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jfks5ULdaqFJ"
      },
      "source": [
        "The we will need to create a new dataframe, for example:\n",
        "\n",
        "```X_categorical_df = pd.DataFrame(X_categorical,\n",
        "                                      columns=encoded_column_names,\n",
        "                                      index=X.index)```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3y_2zkS_aSkq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONAvgAhVXyTY"
      },
      "outputs": [],
      "source": [
        "# Check the head of the categorical data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0ldJrBya1Qh"
      },
      "source": [
        "Finall we will need to combine back togetger, we can use concat from pandas, eg:\n",
        "\n",
        "```pd.concat([A, B], axis=1)```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IUhg1J8yYPhA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6o2xLXTWm1E"
      },
      "source": [
        "Next we will scale our quantitative data.\n",
        "\n",
        "`StandardScaler` standardizes features by removing the mean and scaling them to unit variance. It transforms data so that each feature has a mean of 0 and a standard deviation of 1.\n",
        "\n",
        "**Use Case:** StandardScaler is typically used for algorithms that rely on the scale of data, such as logistic regression, support vector machines, or neural networks.\n",
        "\n",
        "**Documentation:** [StandardScaler - scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n",
        "\n",
        "**IMPORTANT** Before or after training and test split?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qracRD1WX6wQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d49oAseuYSTb"
      },
      "outputs": [],
      "source": [
        "# Create a standard scalar and fit\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ncl2A9xxWm1F"
      },
      "outputs": [],
      "source": [
        "# transform the data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MNG_yFlWYkIZ"
      },
      "outputs": [],
      "source": [
        "# Take a look at the new data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lFz9mqtbToy"
      },
      "source": [
        "Create new dataframes for scaled data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HrVHLPQqY0z4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7q_Ef0B-ZAFo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IwO9nBLbWPy"
      },
      "source": [
        "Finally combine back with one hot encoded data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQOIqluCZFcw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izO7rp66ZuVh"
      },
      "outputs": [],
      "source": [
        "# Check the gead\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eeNvIYeWm1F"
      },
      "source": [
        "### Logistic regression\n",
        "\n",
        "We will now apply the logistic regression model from scikit-learn.\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
        "\n",
        "Take a look at the parameters:\n",
        "\n",
        "* For now we want to set penalty = None, we will talk about regularisation later in the course, which scikit-learn does by default. To ensure this works well we should also set solver to lbfgs and max_iter to 1000.\n",
        "* It's always useful to check how intercepts are dealt with for regression.\n",
        "\n",
        "Now we can initialise our model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ok_8_4kWm1F"
      },
      "outputs": [],
      "source": [
        "# Initialise a Logistic Regression model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cin_3nSyWm1F"
      },
      "source": [
        "Next we fit our model to the data, be careful which data to use:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-CibhDBWm1F"
      },
      "outputs": [],
      "source": [
        "# Fit the model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obQvpmaAWm1F"
      },
      "source": [
        "Next we apply our model to make prediction, in this case on the test set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lCS1jsalWm1F"
      },
      "outputs": [],
      "source": [
        "# generate predictions on the test dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wnbr8_kTWm1F"
      },
      "outputs": [],
      "source": [
        "# Check if our model is making predictions or estimating probabilities\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ofn-vy5Wm1F"
      },
      "source": [
        "Make use of the scikitlearn functions to produce our metrics of interest:\n",
        "\n",
        "```\n",
        "accuracy_score, precision_score, recall_score, f1_score\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUlxuQ7wWm1F"
      },
      "outputs": [],
      "source": [
        "# Calculate evaluation metrics\n",
        "\n",
        "\n",
        "# Print the metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilxEhJoqWm1F"
      },
      "source": [
        "Remember we have the ability to adjust the focus of our classifier. The ROC curve let's us explore our options in this adjustment and let's us compare classifiers for their ability to give us more options. Let's apply\n",
        "```\n",
        "fpr, tpr, thresholds = metrics.roc_curve(y, scores)\n",
        "```\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ijd93qchWm1G"
      },
      "outputs": [],
      "source": [
        "# first we need the class probability estimates\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GBwxTJAWm1G"
      },
      "source": [
        "We also also interested in the area under the curve metric. We can use:\n",
        "\n",
        "```\n",
        "roc_auc_score(y_true, y_score)\n",
        "```\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZTKDJLbWm1G"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNuG3B-yWm1G"
      },
      "source": [
        "Typically we like to visualise this curve, we can use the values we generated or the function:\n",
        "```\n",
        "RocCurveDisplay(fpr= , tpr= , roc_auc= ).plot()\n",
        "```\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.RocCurveDisplay.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rey8JdnpWm1G"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYKojCC1cq-6"
      },
      "source": [
        "We can also explore our model a little further. We can look at the coefficients:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6luVzUlcw70"
      },
      "outputs": [],
      "source": [
        "# Get coefficients and intercept\n",
        "coefficients =\n",
        "intercept =\n",
        "feature_names =\n",
        "\n",
        "# Display coefficients alongside feature names\n",
        "\n",
        "coef_df = pd.DataFrame(coefficients, columns=feature_names)\n",
        "print(\"Coefficients:\\n\", coef_df)\n",
        "print(\"Intercept:\", intercept)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrohPfTvdZEb"
      },
      "source": [
        "We might also want to look at residuals. Residuals in Logistic Regression\n",
        "Residuals in logistic regression are typically calculated as:\n",
        "\n",
        "Residual\n",
        "=\n",
        "Observed Value\n",
        "−\n",
        "Predicted Probability\n",
        "Residual=Observed Value−Predicted Probability\n",
        "\n",
        "Goal: The residual plot should show no clear pattern. Patterns might indicate that the model is not capturing some important relationships in the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGevx0CndbWU"
      },
      "outputs": [],
      "source": [
        "# Calculate residuals\n",
        "residuals = y_test - y_proba  # Observed - Predicted probabilities\n",
        "\n",
        "# Choose a feature for plotting residuals\n",
        "feature = 'Age'\n",
        "x_axis = X_test[feature]\n",
        "\n",
        "# Create a residual plot\n",
        "plt.scatter(x_axis, residuals, alpha=0.7)\n",
        "plt.axhline(0, color='red', linestyle='--')  # Add a horizontal line at 0\n",
        "plt.title(f\"Residuals vs {feature}\")\n",
        "plt.xlabel(feature)\n",
        "plt.ylabel(\"Residuals (Observed - Predicted Probability)\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJmyZIh9d5I7"
      },
      "source": [
        "What to Look For:\n",
        "\n",
        "**No Pattern:** The points should be randomly scattered around the horizontal line at 0. This indicates the model is fitting well.\n",
        "\n",
        "**Systematic Patterns:** If the residuals form a curve or other pattern, it might indicate that:\n",
        "\n",
        "\n",
        "*   A non-linear relationship exists, and a transformation of the feature could help.\n",
        "*   An interaction term or additional feature may be missing.\n",
        "\n",
        "**Heteroscedasticity** If the spread of residuals increases or decreases with the feature's value, the model might not be well-calibrated across the range of the feature.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
