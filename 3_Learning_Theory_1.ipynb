{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4reF3LGsUX87"
      },
      "source": [
        "# Learning theory\n",
        "\n",
        "In machine learning, in supervised learning our goal is often to learn a function $ f $ from input data $ X $ that can predict output (or label) $ Y $.  For instance, given a dataset of house sizes and their prices, we're trying to learn a function $ f $ such that:\n",
        "\n",
        "$$ f(\\text{house size}) = \\text{house price} $$\n",
        "\n",
        "\n",
        "Real-world data is rarely perfect. It often contains noise, which are the random fluctuations that can't be explained by the model. This noise can arise from various sources, such as measurement errors, non-representative samples, or inherent randomness in the system.\n",
        "The total error in our predictions can be broken down into:\n",
        "\n",
        "The prediction error of a model can be decomposed into two components:\n",
        "\n",
        "1. **Reducible Error**: This is the error due to the model itself. By choosing a more appropriate model, tuning hyperparameters, or gathering more data, we can potentially reduce this error.\n",
        "\n",
        "2. **Irreducible Error**: This represents the noise in the data. No matter how good our model is, we can't eliminate this error because it's a result of inherent randomness or other factors beyond our control.\n",
        "\n",
        "Mathematically, the expected mean squared error (MSE) for predictions can be decomposed as:\n",
        "\n",
        "$$ \\text{E}[\\text{MSE}] = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Error} $$\n",
        "\n",
        "Where:\n",
        "\n",
        "- **Bias** is the error introduced by approximating the real-world problem (which may be complex) by a too-simple model.\n",
        "- **Variance** is the error introduced by the model's complexity in trying to fit the data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itZSqNkPUX89"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "from scipy.stats import multivariate_normal\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import PolynomialFeatures, label_binarize\n",
        "from sklearn.utils import resample\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score, auc, mean_squared_error, accuracy_score, precision_score, recall_score, f1_score, ConfusionMatrixDisplay\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4kRt97xVRLC"
      },
      "source": [
        "#### Demonstration:\n",
        "\n",
        "Let's create a simple dataset based on a known function with some added noise. We'll then try to learn this function with a regression model to illustrate the concept of noise and error components.\n",
        "\n",
        "We are going to generate data from a sine curve."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "ZCjHb7J3UX8-",
        "outputId": "e8feb0ce-ea54-4d0e-f795-6eb620e8aa51"
      },
      "outputs": [],
      "source": [
        "# Generate synthetic data\n",
        "np.random.seed(0)\n",
        "X = np.sort(5 * np.random.rand(100, 1), axis=0)\n",
        "y_true = np.sin(X).ravel()\n",
        "y = y_true + np.random.normal(0, 0.8, size=X.shape[0])  # Adding noise\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(X, y, color='darkorange', label='data with noise')\n",
        "plt.plot(X, y_true, color='navy', label='true function')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxvPseQgmY8e"
      },
      "source": [
        "Let's try and fit some models, with different degrees of complexity. We will fit polynomial models with 1, 3, or 20 degrees.\n",
        "\n",
        "Note our model assumption does not match our data generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "6EHhxPONUX8_",
        "outputId": "665d50aa-9c17-4479-e70d-93b33ee1bb9d"
      },
      "outputs": [],
      "source": [
        "degrees = [1, 3, 20]\n",
        "plt.figure(figsize=(14, 5))\n",
        "\n",
        "for i, degree in enumerate(degrees):\n",
        "    ax = plt.subplot(1, len(degrees), i + 1)\n",
        "    plt.setp(ax, xticks=(), yticks=())\n",
        "\n",
        "    polynomial = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
        "    polynomial.fit(X, y)\n",
        "    y_pred = polynomial.predict(X)\n",
        "\n",
        "    plt.scatter(X, y, color='darkorange')\n",
        "    plt.plot(X, y_true, color='navy', label='true function')\n",
        "    plt.plot(X, y_pred, color='green', label='degree %d' % degree)\n",
        "    plt.legend(loc='lower left')\n",
        "    plt.title(f\"Degree {degree}\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exNI70G3UX8_"
      },
      "source": [
        "Certainly. Let's compute the bias, variance, and irreducible error for each of the polynomial models we proposed earlier (degrees 1, 4, and 15).\n",
        "\n",
        "To compute these, we'll follow these steps:\n",
        "\n",
        "1. **Bias^2**:\n",
        "   $$\n",
        "   \\text{Bias}^2(x) = \\left( \\text{E}[ \\hat{f}(x) ] - f(x) \\right)^2\n",
        "   $$\n",
        "   Here, \\( \\hat{f}(x) \\) is the model prediction, and \\( f(x) \\) is the true function. The expectation is over different training sets.\n",
        "   \n",
        "2. **Variance**:\n",
        "   $$\n",
        "   \\text{Var}(x) = \\text{E}\\left[ \\left( \\hat{f}(x) - \\text{E}[ \\hat{f}(x) ] \\right)^2 \\right]\n",
        "   $$\n",
        "   This measures how much the model predictions will vary for a given point \\( x \\) over different training sets.\n",
        "   \n",
        "3. **Irreducible Error**:\n",
        "   Given by the noise in the data. If we assume the noise follows a Gaussian distribution, the irreducible error is its variance.\n",
        "   \n",
        "To estimate these values, we can use a bootstrap method: We'll sample the dataset with replacement multiple times, fit our models to these samples, and then compute the predictions. This will allow us to estimate the expected value and variance of the predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgWptqhQmIrN"
      },
      "outputs": [],
      "source": [
        "results = {}\n",
        "n_bootstrap = 1000\n",
        "degrees = [1, 2, 3, 4, 5, 10, 20]\n",
        "\n",
        "# Pre-allocate arrays large enough for any degree, then reuse them\n",
        "n = len(X)\n",
        "for d in degrees:\n",
        "    # Online computation of mean and variance (Welford) to save memory\n",
        "    mean_pred = np.zeros(n)\n",
        "    m2_pred   = np.zeros(n)\n",
        "\n",
        "    for b in range(1, n_bootstrap + 1):\n",
        "        X_s, y_s = resample(X, y)\n",
        "        model = make_pipeline(\n",
        "            PolynomialFeatures(d, include_bias=False),\n",
        "            LinearRegression()\n",
        "            )\n",
        "        model.fit(X_s, y_s)\n",
        "        y_pred = model.predict(X)          # **in-sample** prediction\n",
        "\n",
        "        delta = y_pred - mean_pred\n",
        "        mean_pred += delta / b\n",
        "        m2_pred   += delta * (y_pred - mean_pred)\n",
        "\n",
        "    var_per_point = m2_pred / (n_bootstrap - 1)\n",
        "    bias_sq_per_point = (mean_pred - y_true) ** 2\n",
        "\n",
        "\n",
        "    results[d] = {\n",
        "        \"Bias²\":        bias_sq_per_point.mean(),\n",
        "        \"Variance\":     var_per_point.mean(),\n",
        "        }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-we2fFSLmN4s",
        "outputId": "e85c578a-3918-4d8a-ee89-ba1a8e4dec6d"
      },
      "outputs": [],
      "source": [
        "# Pretty-print\n",
        "for d, metrics in results.items():\n",
        "    print(f\"deg {d:2d}: \"\n",
        "          f\"bias²={metrics['Bias²']:.4f}, \"\n",
        "          f\"var={metrics['Variance']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwIvPH1-UX9C"
      },
      "source": [
        "### Question - Which Model Will do Best?\n",
        "\n",
        "Let's look again at fitting polynomials to our data. Here we will plot our errors for a range of polynomial degrees (levels of complexity)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jiE16STiUX9C"
      },
      "outputs": [],
      "source": [
        "# Evaluating polynomial regression models of varying complexity on training and test datasets\n",
        "def evaluate_poly_models(X_train, y_train, poly_degrees):\n",
        "    evaluate_poly_models_split(X_train, y_train, X_train, y_train, poly_degrees)\n",
        "\n",
        "def evaluate_poly_models_split(X_train, y_train, X_test, y_test, poly_degrees):\n",
        "    train_errors = []\n",
        "    test_errors = []\n",
        "\n",
        "    # Range of polynomial degrees\n",
        "    degrees = list(range(1, poly_degrees + 1))\n",
        "\n",
        "    for degree in degrees:\n",
        "        # Create and fit the polynomial regression model\n",
        "        polynomial = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
        "        polynomial.fit(X_train, y_train)\n",
        "\n",
        "        # Predict on training and test datasets\n",
        "        y_train_pred = polynomial.predict(X_train)\n",
        "        y_test_pred = polynomial.predict(X_test)\n",
        "\n",
        "        # Compute the Mean Squared Error (MSE) for training and test datasets\n",
        "        train_mse = mean_squared_error(y_train, y_train_pred)\n",
        "        test_mse = mean_squared_error(y_test, y_test_pred)\n",
        "\n",
        "        train_errors.append(train_mse)\n",
        "        test_errors.append(test_mse)\n",
        "\n",
        "    # Plotting the training and test errors as a function of model complexity\n",
        "    plt.figure(figsize=(12, 7))\n",
        "    plt.plot(degrees, train_errors, label='Training Error', marker='o')\n",
        "    plt.plot(degrees, test_errors, label='Test Error', marker='o')\n",
        "    plt.xlabel('Degree of Polynomial')\n",
        "    plt.ylabel('Mean Squared Error (MSE)')\n",
        "    plt.title('Training vs. Test Error')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "id": "vAv_Z81mUX9D",
        "outputId": "79f8dd14-e042-4eae-c7c4-89f20cb1e273"
      },
      "outputs": [],
      "source": [
        "evaluate_poly_models(X, y, 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjdWIDtcUX9D"
      },
      "source": [
        "The more complex the model the better right? What's the problem?\n",
        "\n",
        "\n",
        "We are testing on our training data, we need our training test splits:\n",
        "\n",
        "```\n",
        "train_test_split(X, y, test_size= ... , random_state= ...)\n",
        "```\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "peV4gczoUX9D"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3_GNJZ8n8rV"
      },
      "source": [
        "Use shape to take a look at you new data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMHovy8toJBM",
        "outputId": "5ad95143-21ad-45ed-c3df-a6c374043045"
      },
      "outputs": [],
      "source": [
        "X_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHUld1D4oJgk"
      },
      "source": [
        "Let's try again, this time making our training and test on different models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "id": "6hCd_5VHUX9D",
        "outputId": "23d121aa-9ba0-4b76-9d5c-fda71c4b00ac"
      },
      "outputs": [],
      "source": [
        "evaluate_poly_models_split(X_train, y_train, X_test, y_test, 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dr3ia4alUX9D"
      },
      "source": [
        "### Question - Ok we need a test set, but why not train on all the data?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hAJ5yVw_UX9D"
      },
      "outputs": [],
      "source": [
        "evaluate_poly_models_split(X, y, X_test, y_test, 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwaPWXEhUX9D"
      },
      "source": [
        "This is called data leakage - when the model has learned on data in the the training set. This is one of the easy ways to feel over confident about your model ability (overfit) without being able to detect it.\n",
        "\n",
        "In our case it's easy to spot how this happened, but it can be harder in practice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction to gradient descent and machine learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Gradient Descent is an optimization algorithm that's used to minimize some function by iteratively moving in the direction of steepest decrease, which is the negative gradient of the function. In the context of machine learning, this function we're trying to minimize is often a loss function that measures how well our model's predictions match the true values.\n",
        "\n",
        "For a linear regression problem, our model is:\n",
        "\n",
        "$$ f(x) = w \\cdot x + b $$\n",
        "\n",
        "Where:\n",
        "- \\( w \\) is the weight (or coefficient).\n",
        "- \\( b \\) is the bias (or intercept).\n",
        "\n",
        "The objective is to find the best values for \\( w \\) and \\( b \\) that minimize the Mean Squared Error (MSE) between the predictions \\( f(x) \\) and the true values \\( y \\).\n",
        "\n",
        "### 2. Generating Synthetic Data\n",
        "\n",
        "We'll generate synthetic data that roughly follows a linear trend to illustrate this concept."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generating synthetic data\n",
        "np.random.seed(42)\n",
        "X_linear = 2 * np.random.rand(100, 1)\n",
        "y_linear = 4 + 3 * X_linear + np.random.randn(100, 1)\n",
        "\n",
        "plt.scatter(X_linear, y_linear)\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.title(\"Synthetic Linear Data\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "The idea behind gradient descent is to update the model parameters (in this case, \\( w \\) and \\( b \\)) iteratively to minimize the MSE. The updates are done in the direction of the steepest decrease of the MSE.\n",
        "\n",
        "For each parameter \\( \\theta \\) (which could be \\( w \\) or \\( b \\)), the update rule is:\n",
        "\n",
        "$ \\theta = \\theta - \\alpha \\times \\nabla_\\theta \\text{MSE}(\\theta) $\n",
        "\n",
        "Where:\n",
        "- $ \\alpha $ is the learning rate, a hyperparameter that determines the step size at each iteration.\n",
        "- $ \\nabla_\\theta \\text{MSE}(\\theta) $ is the gradient of the MSE with respect to the parameter $ \\theta $.\n",
        "\n",
        "\n",
        "\n",
        "#### Gradient Descent for Linear Regression\n",
        "\n",
        "Before implementing gradient descent, let's establish the objective function: the Mean Squared Error (MSE). For \\( m \\) data points, the MSE is given by:\n",
        "\n",
        "$$\n",
        "\\text{MSE} = \\frac{1}{m} \\sum_{i=1}^{m} (\\hat{y}_i - y_i)^2\n",
        "$$\n",
        "\n",
        "Where $ \\hat{y}_i $ is the predicted value for the $ i $-th data point, and $ y_i $ is the true value. The predicted value is:\n",
        "\n",
        "$$\n",
        "\\hat{y}_i = w \\cdot x_i + b\n",
        "$$\n",
        "\n",
        "To perform gradient descent, we need the gradient of the MSE with respect to the model parameters \\( w \\) and \\( b \\). The update rules are:\n",
        "\n",
        "$$\n",
        "w = w - \\alpha \\times \\frac{2}{m} \\sum_{i=1}^{m} (\\hat{y}_i - y_i) x_i\n",
        "$$\n",
        "\n",
        "$$\n",
        "b = b - \\alpha \\times \\frac{2}{m} \\sum_{i=1}^{m} (\\hat{y}_i - y_i)\n",
        "$$\n",
        "\n",
        "Where $ \\alpha $ is the learning rate.\n",
        "\n",
        "Take a look how we implement below:\n",
        "\n",
        "```\n",
        "    # Predictions\n",
        "    y_pred = w_current * X + b_current\n",
        "\n",
        "    # Calculate MSE and store\n",
        "    mse = compute_mse(y, y_pred)\n",
        "\n",
        "    # Gradients\n",
        "    gradient_w = (2/len(X)) * X.T.dot(y_pred - y)\n",
        "    gradient_b = 2 * (y_pred - y).mean()\n",
        "\n",
        "    # Update rules\n",
        "    w_current -= learning_rate * gradient_w[0][0]  \n",
        "    b_current -= learning_rate * gradient_b\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mse_history_manual = []\n",
        "w_history = [np.random.randn()]\n",
        "b_history = [np.random.randn()]\n",
        "gradient_w_history = []\n",
        "gradient_b_history = []\n",
        "\n",
        "def reset_grad_globals():\n",
        "    global w_history, b_history, mse_history_manual, gradient_w_history, gradient_b_history\n",
        "\n",
        "    mse_history_manual = []\n",
        "    w_history = [np.random.randn()]\n",
        "    b_history = [np.random.randn()]\n",
        "    gradient_w_history = []\n",
        "    gradient_b_history = []\n",
        "\n",
        "def compute_mse(y_true, y_pred):\n",
        "    \"\"\"Compute Mean Squared Error.\"\"\"\n",
        "    return ((y_pred - y_true) ** 2).mean()\n",
        "\n",
        "def manual_gradient_descent_step(X, y, learning_rate):\n",
        "    global w_history, b_history, mse_history_manual, gradient_w_history, gradient_b_history\n",
        "\n",
        "    # Current weight and bias\n",
        "    w_current = w_history[-1]\n",
        "    b_current = b_history[-1]\n",
        "\n",
        "    # Predictions\n",
        "    y_pred = w_current * X + b_current\n",
        "\n",
        "    # Calculate MSE and store\n",
        "    mse = compute_mse(y, y_pred)\n",
        "    mse_history_manual.append(mse)\n",
        "\n",
        "    # Gradients\n",
        "    gradient_w = (2 / len(X)) * X.T.dot(y_pred - y)\n",
        "    gradient_b = 2 * (y_pred - y).mean()\n",
        "\n",
        "    # Store gradients\n",
        "    gradient_w_history.append(gradient_w[0][0])\n",
        "    gradient_b_history.append(gradient_b)\n",
        "\n",
        "    # Update rules\n",
        "    w_current -= learning_rate * gradient_w[0][0]\n",
        "    b_current -= learning_rate * gradient_b\n",
        "\n",
        "    # Store updated parameters\n",
        "    w_history.append(w_current)\n",
        "    b_history.append(b_current)\n",
        "\n",
        "    # Plot the data vs. linear model and histories in a 2x2 grid\n",
        "    fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
        "\n",
        "    # Data vs. linear model\n",
        "    axs[0, 0].scatter(X, y, label=\"Data points\")\n",
        "    axs[0, 0].plot(X, w_current * X + b_current, color=\"red\", label=\"Current Learned Linear Model\")\n",
        "    axs[0, 0].set_title('Data vs. Linear Model')\n",
        "    axs[0, 0].set_xlabel('X')\n",
        "    axs[0, 0].set_ylabel('y')\n",
        "    axs[0, 0].legend()\n",
        "    axs[0, 0].grid(True)\n",
        "\n",
        "    # If this is the first iteration, only show the data plot\n",
        "    if len(mse_history_manual) > 1:\n",
        "        # MSE history\n",
        "        axs[0, 1].plot(mse_history_manual, 'r')\n",
        "        axs[0, 1].set_title('MSE History')\n",
        "        axs[0, 1].set_xlabel('Iterations')\n",
        "        axs[0, 1].set_ylabel('MSE')\n",
        "\n",
        "        # Parameter values history\n",
        "        axs[1, 0].plot(w_history, 'g', label=\"Weight (w)\")\n",
        "        axs[1, 0].plot(b_history, 'b', label=\"Bias (b)\")\n",
        "        axs[1, 0].set_title('Parameter Values History')\n",
        "        axs[1, 0].set_xlabel('Iterations')\n",
        "        axs[1, 0].set_ylabel('Parameter Value')\n",
        "        axs[1, 0].legend()\n",
        "\n",
        "        # Gradients history (combined)\n",
        "        axs[1, 1].plot(gradient_w_history, 'g', label=\"Gradient w.r.t Weight (w)\")\n",
        "        axs[1, 1].plot(gradient_b_history, 'b', label=\"Gradient w.r.t Bias (b)\")\n",
        "        axs[1, 1].set_title('Gradients History')\n",
        "        axs[1, 1].set_xlabel('Iterations')\n",
        "        axs[1, 1].set_ylabel('Gradient Value')\n",
        "        axs[1, 1].legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use this cell to reset gradient descent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reset_grad_globals()\n",
        "# Return initial values for confirmation\n",
        "w_history[-1], b_history[-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use this cell to step through gradient descent. Explore the use of different learning rates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "manual_gradient_descent_step(X_linear, y_linear, learning_rate=0.9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Take a look at how it works on our previous data which was not linear."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reset_grad_globals()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "manual_gradient_descent_step(X, y, learning_rate=0.02)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
