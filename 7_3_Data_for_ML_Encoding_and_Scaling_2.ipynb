{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8e9ec725",
      "metadata": {
        "id": "8e9ec725"
      },
      "source": [
        "## Encoding, Transforming, and Scaling Features\n",
        "\n",
        "This tutorial is based upon the textbook:\n",
        "\n",
        "Walker, M. (2022). Data Cleaning and Exploration with Machine Learning. Pakt Publishing Ltd..\n",
        "\n",
        "Typically, machine learning algorithms require some form of encoding of variables.\n",
        "Additionally, our models often perform better with scaling so that features with higher\n",
        "variability do not overwhelm the optimization. We will show you how to use different\n",
        "scaling techniques when your features have dramatically different ranges."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "3109d7d9",
      "metadata": {
        "id": "3109d7d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: feature-engine in c:\\my-git\\datascience-novaims\\machinelearning\\.venv\\lib\\site-packages (1.9.3)\n",
            "Requirement already satisfied: category_encoders in c:\\my-git\\datascience-novaims\\machinelearning\\.venv\\lib\\site-packages (2.9.0)\n",
            "Requirement already satisfied: numpy>=1.18.2 in c:\\my-git\\datascience-novaims\\machinelearning\\.venv\\lib\\site-packages (from feature-engine) (2.3.5)\n",
            "Requirement already satisfied: pandas>=2.2.0 in c:\\my-git\\datascience-novaims\\machinelearning\\.venv\\lib\\site-packages (from feature-engine) (2.3.3)\n",
            "Requirement already satisfied: scikit-learn>=1.4.0 in c:\\my-git\\datascience-novaims\\machinelearning\\.venv\\lib\\site-packages (from feature-engine) (1.7.2)\n",
            "Requirement already satisfied: scipy>=1.4.1 in c:\\my-git\\datascience-novaims\\machinelearning\\.venv\\lib\\site-packages (from feature-engine) (1.16.3)\n",
            "Requirement already satisfied: statsmodels>=0.11.1 in c:\\my-git\\datascience-novaims\\machinelearning\\.venv\\lib\\site-packages (from feature-engine) (0.14.5)\n",
            "Requirement already satisfied: patsy>=0.5.1 in c:\\my-git\\datascience-novaims\\machinelearning\\.venv\\lib\\site-packages (from category_encoders) (1.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\my-git\\datascience-novaims\\machinelearning\\.venv\\lib\\site-packages (from pandas>=2.2.0->feature-engine) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\my-git\\datascience-novaims\\machinelearning\\.venv\\lib\\site-packages (from pandas>=2.2.0->feature-engine) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\my-git\\datascience-novaims\\machinelearning\\.venv\\lib\\site-packages (from pandas>=2.2.0->feature-engine) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in c:\\my-git\\datascience-novaims\\machinelearning\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=2.2.0->feature-engine) (1.17.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\my-git\\datascience-novaims\\machinelearning\\.venv\\lib\\site-packages (from scikit-learn>=1.4.0->feature-engine) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\my-git\\datascience-novaims\\machinelearning\\.venv\\lib\\site-packages (from scikit-learn>=1.4.0->feature-engine) (3.6.0)\n",
            "Requirement already satisfied: packaging>=21.3 in c:\\my-git\\datascience-novaims\\machinelearning\\.venv\\lib\\site-packages (from statsmodels>=0.11.1->feature-engine) (25.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install feature-engine category_encoders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "f58b3ecd",
      "metadata": {
        "id": "f58b3ecd"
      },
      "outputs": [],
      "source": [
        "# import pandas, numpy, and matplotlib\n",
        "import pandas as pd\n",
        "import category_encoders as ce\n",
        "from sklearn.model_selection import train_test_split\n",
        "import feature_engine.selection as fesel\n",
        "from feature_engine.encoding import OneHotEncoder\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from category_encoders.hashing import HashingEncoder\n",
        "\n",
        "pd.options.display.float_format = '{:,.2f}'.format\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f7f41127",
      "metadata": {
        "id": "f7f41127"
      },
      "outputs": [],
      "source": [
        "nls97 = pd.read_csv(\"nls97b.csv\")\n",
        "nls97.set_index(\"personid\", inplace=True)\n",
        "\n",
        "ltpoland = pd.read_csv(\"ltpoland.csv\")\n",
        "ltpoland.set_index(\"station\", inplace=True)\n",
        "ltpoland.dropna(inplace=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f2c11e1",
      "metadata": {
        "id": "7f2c11e1"
      },
      "source": [
        "create training and testing DataFrames for the features (X_train and\n",
        "X_test) and the targets (y_train and y_test). In this example, wageincome\n",
        "is the target variable. We set the test_size parameter to 0.3 to leave 30%\n",
        "of the observations for testing. Note that we will only work with the Scholastic\n",
        "Assessment Test (SAT) and grade point average (GPA) data from the NLS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "0bd68ac9",
      "metadata": {
        "id": "0bd68ac9"
      },
      "outputs": [],
      "source": [
        "feature_cols = ['satverbal','satmath','gpascience',\n",
        "  'gpaenglish','gpamath','gpaoverall']\n",
        "\n",
        "# separate NLS data into train and test datasets\n",
        "X_train, X_test, y_train, y_test =  \\\n",
        "  train_test_split(nls97[feature_cols],\\\n",
        "  nls97[['wageincome']], test_size=0.3, random_state=0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "3ed5d732",
      "metadata": {
        "id": "3ed5d732"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 6288 entries, 574974 to 370933\n",
            "Data columns (total 6 columns):\n",
            " #   Column      Non-Null Count  Dtype  \n",
            "---  ------      --------------  -----  \n",
            " 0   satverbal   1001 non-null   float64\n",
            " 1   satmath     1001 non-null   float64\n",
            " 2   gpascience  3998 non-null   float64\n",
            " 3   gpaenglish  4078 non-null   float64\n",
            " 4   gpamath     4056 non-null   float64\n",
            " 5   gpaoverall  4223 non-null   float64\n",
            "dtypes: float64(6)\n",
            "memory usage: 343.9 KB\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 6288 entries, 574974 to 370933\n",
            "Data columns (total 1 columns):\n",
            " #   Column      Non-Null Count  Dtype  \n",
            "---  ------      --------------  -----  \n",
            " 0   wageincome  3599 non-null   float64\n",
            "dtypes: float64(1)\n",
            "memory usage: 98.2 KB\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 2696 entries, 363170 to 629736\n",
            "Data columns (total 6 columns):\n",
            " #   Column      Non-Null Count  Dtype  \n",
            "---  ------      --------------  -----  \n",
            " 0   satverbal   405 non-null    float64\n",
            " 1   satmath     406 non-null    float64\n",
            " 2   gpascience  1686 non-null   float64\n",
            " 3   gpaenglish  1720 non-null   float64\n",
            " 4   gpamath     1710 non-null   float64\n",
            " 5   gpaoverall  1781 non-null   float64\n",
            "dtypes: float64(6)\n",
            "memory usage: 147.4 KB\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 2696 entries, 363170 to 629736\n",
            "Data columns (total 1 columns):\n",
            " #   Column      Non-Null Count  Dtype  \n",
            "---  ------      --------------  -----  \n",
            " 0   wageincome  1492 non-null   float64\n",
            "dtypes: float64(1)\n",
            "memory usage: 42.1 KB\n"
          ]
        }
      ],
      "source": [
        "X_train.info()\n",
        "y_train.info()\n",
        "X_test.info()\n",
        "y_test.info()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42713074",
      "metadata": {
        "id": "42713074"
      },
      "source": [
        "## Removing redundant or unhelpful features\n",
        "\n",
        "During the process of data cleaning and manipulation, we often end up with data that\n",
        "is no longer meaningful. Perhaps we subsetted data based on a single feature value, and\n",
        "we have retained that feature even though it now has the same value for all observations.\n",
        "Or, for the subset of the data that we are using, two features have the same value. Ideally,\n",
        "we catch those redundancies during our data cleaning. However, if we do not catch them\n",
        "during that process, we can use the open source feature-engine package to help us.\n",
        "\n",
        "Additionally, there might be features that are so highly correlated that it is very unlikely\n",
        "that we could build a model that could use all of them effectively. feature-engine has\n",
        "a method, DropCorrelatedFeatures, that makes it easy to remove a feature when it\n",
        "is highly correlated with another feature.\n",
        "\n",
        "### Warning - you are dropping data without testing it's usefulness to your model. Shown for demonstration but be careful you are not losing valuable inofmration.\n",
        "\n",
        "Here we will work with land temperature data, along with the NLS data. Note\n",
        "that we will only load temperature data for Poland here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "82d94462",
      "metadata": {
        "id": "82d94462"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>satverbal</th>\n",
              "      <th>satmath</th>\n",
              "      <th>gpascience</th>\n",
              "      <th>gpaenglish</th>\n",
              "      <th>gpamath</th>\n",
              "      <th>gpaoverall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>satverbal</th>\n",
              "      <td>1.00</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>satmath</th>\n",
              "      <td>0.73</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpascience</th>\n",
              "      <td>0.44</td>\n",
              "      <td>0.48</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.79</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpaenglish</th>\n",
              "      <td>0.44</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.67</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.84</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpamath</th>\n",
              "      <td>0.38</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.60</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gpaoverall</th>\n",
              "      <td>0.42</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.84</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            satverbal  satmath  gpascience  gpaenglish  gpamath  gpaoverall\n",
              "satverbal        1.00     0.73        0.44        0.44     0.38        0.42\n",
              "satmath          0.73     1.00        0.48        0.43     0.52        0.48\n",
              "gpascience       0.44     0.48        1.00        0.67     0.61        0.79\n",
              "gpaenglish       0.44     0.43        0.67        1.00     0.60        0.84\n",
              "gpamath          0.38     0.52        0.61        0.60     1.00        0.75\n",
              "gpaoverall       0.42     0.48        0.79        0.84     0.75        1.00"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# remove a feature highly correlated with another\n",
        "X_train.corr()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3bebb08",
      "metadata": {
        "id": "e3bebb08"
      },
      "source": [
        "Let's drop features that have a correlation higher than 0.75 with another feature.\n",
        "We pass 0.75 to the threshold parameter of DropCorrelatedFeatures,\n",
        "indicating that we want to use Pearson coefficients and that we want to evaluate\n",
        "all the features by setting the variables to None. We use the fit method on the\n",
        "training data and then transform both the training and testing data. The info\n",
        "method shows that the resulting training DataFrame (X_train_tr) has all of\n",
        "the features except gpaoverall, which has correlations of 0.793 and 0.844 with\n",
        "gpascience and gpaenglish, respectively (DropCorrelatedFeatures will\n",
        "evaluate from left to right, so if gpamath and gpaoverall are highly correlated,\n",
        "it will drop gpaoverall. If gpaoverall had been to the left of gpamath, it\n",
        "would have dropped gpamath):\n",
        "\n",
        "https://feature-engine.trainindata.com/en/latest/user_guide/selection/DropCorrelatedFeatures.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "1a283ea5",
      "metadata": {
        "id": "1a283ea5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 6288 entries, 574974 to 370933\n",
            "Data columns (total 5 columns):\n",
            " #   Column      Non-Null Count  Dtype  \n",
            "---  ------      --------------  -----  \n",
            " 0   satverbal   1001 non-null   float64\n",
            " 1   satmath     1001 non-null   float64\n",
            " 2   gpascience  3998 non-null   float64\n",
            " 3   gpaenglish  4078 non-null   float64\n",
            " 4   gpamath     4056 non-null   float64\n",
            "dtypes: float64(5)\n",
            "memory usage: 294.8 KB\n"
          ]
        }
      ],
      "source": [
        "tr = fesel.DropCorrelatedFeatures(variables=None, method='pearson', threshold=0.75)\n",
        "tr.fit(X_train)\n",
        "X_train_tr = tr.transform(X_train)\n",
        "X_test_tr = tr.transform(X_test)\n",
        "X_train_tr.info()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f967618a",
      "metadata": {
        "id": "f967618a"
      },
      "source": [
        "Let's drop features that have the same values as other features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "045b5a86",
      "metadata": {
        "id": "045b5a86"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>satverbal</th>\n",
              "      <th>satmath</th>\n",
              "      <th>gpascience</th>\n",
              "      <th>gpaenglish</th>\n",
              "      <th>gpamath</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>personid</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>574974</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100.00</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>894733</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>452383</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>670866</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>353165</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>190.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          satverbal  satmath  gpascience  gpaenglish  gpamath\n",
              "personid                                                     \n",
              "574974          NaN      NaN         NaN      100.00      NaN\n",
              "894733          NaN      NaN         NaN         NaN      NaN\n",
              "452383          NaN      NaN         NaN         NaN      NaN\n",
              "670866          NaN      NaN         NaN         NaN      NaN\n",
              "353165          NaN      NaN         NaN         NaN   190.00"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# drop features that have the same values as another feature\n",
        "tr = fesel.DropDuplicateFeatures()\n",
        "tr.fit(X_train_tr)\n",
        "X_train_tr = tr.transform(X_train_tr)\n",
        "X_test_tr = tr.transform(X_test_tr)\n",
        "X_train_tr.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0266627",
      "metadata": {
        "id": "f0266627"
      },
      "source": [
        "## Removing features that leak data!\n",
        "\n",
        "* **Data leakage** happens when a model has access to information during training that would not be available when making predictions in production.\n",
        "* It makes your offline results look “too good to be true” but leads to poor real-world performance.\n",
        "\n",
        "**Example:** predicting loan repayment at the time of application. If you include a feature like “number of missed payments,” that information only becomes available *after* the loan is issued — so your model is secretly peeking into the future.\n",
        "\n",
        "### Common Types of Leakage\n",
        "\n",
        "* **Target leakage:** features that directly or indirectly contain the answer.\n",
        "  *Example: using “loan\\_status” to predict default.*\n",
        "* **Temporal leakage (“time travel”):** features include data from after the prediction point.\n",
        "  *Example: collections activity after the loan application.*\n",
        "* **Preprocessing leakage:** fitting scalers, imputers, or encoders on the full dataset instead of only the training portion.\n",
        "* **Entity leakage:** the same customer or loan appears in both train and validation splits, letting the model memorize.\n",
        "* **Join leakage:** merging on the “latest snapshot” without restricting to information available at the prediction time.\n",
        "\n",
        "### How to Detect Leakage\n",
        "\n",
        "* **Check feature timestamps**: ask, “Was this known *as of prediction time*?” Anything later leaks.\n",
        "* **Monitor performance jumps**: if a single feature or small group suddenly boosts accuracy/AUC to unrealistic levels, it may be leaking.\n",
        "* **Inspect top features**: use feature importance or SHAP to see if suspicious “future” features dominate.\n",
        "* **Cross-validate carefully**: if validation scores are much higher with random splits than with time-based splits, leakage is likely.\n",
        "* **Audit your joins**: make sure you only join records that existed before the prediction timestamp.\n",
        "\n",
        "### Best Practices to Prevent Leakage\n",
        "\n",
        "* **Anchor to a timeline:** define an explicit “as-of date” for every observation, and only use features known at or before that time.\n",
        "* **Use time-based splits:** validate on future data, not random samples.\n",
        "* **Point-in-time joins:** when combining tables, only keep rows recorded before the as-of date.\n",
        "* **Fit preprocessing on train only:** scaling, encoding, and imputing should learn from training folds only.\n",
        "* **Use group-aware splits:** keep the same customer or entity entirely in train or test, not both.\n",
        "* **Handle target encoding safely:** compute mean encoding within cross-validation folds, not across the whole dataset.\n",
        "\n",
        "### Example: Loan Default Prediction\n",
        "\n",
        "**Allowed features (safe):**\n",
        "\n",
        "* Customer’s income at application.\n",
        "* Credit score from bureau snapshot before application date.\n",
        "* Number of past loans repaid or defaulted *before* application.\n",
        "\n",
        "**Leaking features (unsafe):**\n",
        "\n",
        "* Whether the first installment was paid on time.\n",
        "* Collections calls after the loan was granted.\n",
        "* Charge-off date or recovery amounts.\n",
        "\n",
        "Always ask: *Would this information be known at prediction time in production?*\n",
        "* If not, remove or adjust the feature.\n",
        "* Use time-aware splits and point-in-time joins to enforce this rule.\n",
        "\n",
        "Data leakage is one of the most common reasons for models failing in production, but it can be detected with careful thinking about timelines and feature sources.\n",
        "\n",
        "\n",
        "## Encoding Categorical features\n",
        "\n",
        "1. **Most ML algorithms need numbers**\n",
        "   Models like regression, trees, and neural networks can’t work directly with raw text or categories. They require numeric inputs.\n",
        "\n",
        "2. **Numbers ≠ categories**\n",
        "   If we code `female = 1` and `male = 2`, the model may mistakenly think \"male is greater than female.\" Encoding makes it clear these are categories, not numeric scales.\n",
        "\n",
        "3. **Ordinal features matter**\n",
        "   Some categories *do* have an order (e.g., “low”, “medium”, “high”). Encoding should preserve that order so the model understands the ranking.\n",
        "\n",
        "4. **High cardinality**\n",
        "   If a categorical variable has many unique values (e.g., ZIP codes), we may need special strategies to reduce complexity.\n",
        "\n",
        "## One-Hot Encoding\n",
        "\n",
        "Turn each category into its own binary (0/1) column.\n",
        "* Example: Feature `letter` with values `{A, B, C}` becomes three columns:\n",
        "\n",
        "  * `letter_A`: 1 if A, 0 otherwise\n",
        "  * `letter_B`: 1 if B, 0 otherwise\n",
        "  * `letter_C`: 1 if C, 0 otherwise\n",
        "\n",
        "These binary columns are often called **dummy variables**.\n",
        "\n",
        "#### When to Use\n",
        "\n",
        "* **Low-cardinality categorical features** (roughly ≤ 15 categories).\n",
        "* Examples: gender, education level, marital status.\n",
        "\n",
        "For ordinal features (with meaningful ranking), we may use **ordinal encoding** instead.\n",
        "For high-cardinality features, we need other methods (covered later).\n",
        "\n",
        "Encoding ensures that categorical variables are represented in a way that ML algorithms can use. One-hot encoding is the most common approach for variables with a small set of categories, because it’s simple and preserves category identity without implying false numeric relationships.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "293ae1c9",
      "metadata": {
        "id": "293ae1c9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gender</th>\n",
              "      <th>maritalstatus</th>\n",
              "      <th>colenroct99</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>personid</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>736081</th>\n",
              "      <td>Female</td>\n",
              "      <td>Married</td>\n",
              "      <td>1. Not enrolled</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>832734</th>\n",
              "      <td>Male</td>\n",
              "      <td>Never-married</td>\n",
              "      <td>1. Not enrolled</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>453537</th>\n",
              "      <td>Male</td>\n",
              "      <td>Married</td>\n",
              "      <td>1. Not enrolled</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>322059</th>\n",
              "      <td>Female</td>\n",
              "      <td>Divorced</td>\n",
              "      <td>1. Not enrolled</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>324323</th>\n",
              "      <td>Female</td>\n",
              "      <td>Married</td>\n",
              "      <td>2. 2-year college</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          gender  maritalstatus         colenroct99\n",
              "personid                                           \n",
              "736081    Female        Married     1. Not enrolled\n",
              "832734      Male  Never-married     1. Not enrolled\n",
              "453537      Male        Married     1. Not enrolled\n",
              "322059    Female       Divorced     1. Not enrolled\n",
              "324323    Female        Married  2. 2-year college "
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "feature_cols =['gender','maritalstatus','colenroct99']\n",
        "nls97demo = nls97[['wageincome'] + feature_cols].dropna()\n",
        "X_demo_train, X_demo_test, y_demo_train, y_demo_test= train_test_split(nls97demo[feature_cols],\\\n",
        "nls97demo[['wageincome']], test_size=0.3,random_state=0)\n",
        "X_demo_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "cebfa710",
      "metadata": {
        "id": "cebfa710"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>colenroct99</th>\n",
              "      <th>gender_Female</th>\n",
              "      <th>maritalstatus_Married</th>\n",
              "      <th>maritalstatus_Never-married</th>\n",
              "      <th>maritalstatus_Divorced</th>\n",
              "      <th>maritalstatus_Separated</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>personid</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>653737</th>\n",
              "      <td>1. Not enrolled</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>473759</th>\n",
              "      <td>1. Not enrolled</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>348093</th>\n",
              "      <td>1. Not enrolled</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>163509</th>\n",
              "      <td>1. Not enrolled</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>895873</th>\n",
              "      <td>1. Not enrolled</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              colenroct99  gender_Female  maritalstatus_Married  \\\n",
              "personid                                                          \n",
              "653737    1. Not enrolled              1                      1   \n",
              "473759    1. Not enrolled              1                      0   \n",
              "348093    1. Not enrolled              0                      1   \n",
              "163509    1. Not enrolled              1                      1   \n",
              "895873    1. Not enrolled              1                      1   \n",
              "\n",
              "          maritalstatus_Never-married  maritalstatus_Divorced  \\\n",
              "personid                                                        \n",
              "653737                              0                       0   \n",
              "473759                              1                       0   \n",
              "348093                              0                       0   \n",
              "163509                              0                       0   \n",
              "895873                              0                       0   \n",
              "\n",
              "          maritalstatus_Separated  \n",
              "personid                           \n",
              "653737                          0  \n",
              "473759                          0  \n",
              "348093                          0  \n",
              "163509                          0  \n",
              "895873                          0  "
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ohe = OneHotEncoder(drop_last=True,\n",
        "variables=['gender','maritalstatus'])\n",
        "ohe.fit(X_demo_train)\n",
        "X_demo_train_ohe = ohe.transform(X_demo_train)\n",
        "X_demo_test_ohe = ohe.transform(X_demo_test)\n",
        "\n",
        "X_demo_test_ohe.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fc72a06",
      "metadata": {
        "id": "3fc72a06"
      },
      "source": [
        "Categorical features can be either nominal or ordinal, as discussed in Chapter 1,\n",
        "Examining the Distribution of Features and Targets. Gender and marital status are nominal.\n",
        "Their values do not imply order. For example, \"never married\" is not a higher value\n",
        "than \"divorced.\"\n",
        "\n",
        "However, when a categorical feature is ordinal, we want the encoding to capture the\n",
        "ranking of the values. For example, if we have a feature that has the values of low, medium,\n",
        "and high, one-hot encoding would lose this ordering. Instead, a transformed feature with\n",
        "the values of 1, 2, and 3 for low, medium, and high, respectively, would be better. We can\n",
        "accomplish this with ordinal encoding.\n",
        "\n",
        "The college enrollment feature on the NLS dataset can be considered an ordinal feature.\n",
        "The values range from 1. Not enrolled to 3. 4-year college. We should use ordinal encoding\n",
        "to prepare it for modeling. We will do that next:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "435a30f6",
      "metadata": {
        "id": "435a30f6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['1. Not enrolled', '2. 2-year college ', '3. 4-year college']]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# OrdinalEncoder expects you to pass categories as a list of lists:\n",
        "# Each inner list contains the allowed categories in the order you want them encoded.\n",
        "# Using .unique() directly gives a NumPy array, but sklearn requires a list.\n",
        "# We also need to wrap it inside another list because we're encoding ONE column here.\n",
        "\n",
        "categories = [list(X_demo_train['colenroct99'].dropna().unique())]\n",
        "\n",
        "categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "3f124070",
      "metadata": {
        "id": "3f124070"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gender_Female</th>\n",
              "      <th>maritalstatus_Married</th>\n",
              "      <th>maritalstatus_Never-married</th>\n",
              "      <th>maritalstatus_Divorced</th>\n",
              "      <th>maritalstatus_Separated</th>\n",
              "      <th>colenroct99</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>personid</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>736081</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>832734</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>453537</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>322059</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>324323</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          gender_Female  maritalstatus_Married  maritalstatus_Never-married  \\\n",
              "personid                                                                      \n",
              "736081                1                      1                            0   \n",
              "832734                0                      0                            1   \n",
              "453537                0                      1                            0   \n",
              "322059                1                      0                            0   \n",
              "324323                1                      1                            0   \n",
              "\n",
              "          maritalstatus_Divorced  maritalstatus_Separated  colenroct99  \n",
              "personid                                                                \n",
              "736081                         0                        0         0.00  \n",
              "832734                         0                        0         0.00  \n",
              "453537                         0                        0         0.00  \n",
              "322059                         1                        0         0.00  \n",
              "324323                         0                        0         1.00  "
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# -----------------------------------------------\n",
        "# Step 2: Initialize the encoder with fixed categories\n",
        "# -----------------------------------------------\n",
        "oe = OrdinalEncoder(categories=categories)\n",
        "\n",
        "# -----------------------------------------------\n",
        "# Step 3: Fit *only on training data*, then transform\n",
        "# -----------------------------------------------\n",
        "colenr_enc_train = pd.DataFrame(\n",
        "    oe.fit_transform(X_demo_train_ohe[['colenroct99']]),  # fit + transform\n",
        "    columns=['colenroct99'],\n",
        "    index=X_demo_train_ohe.index\n",
        ")\n",
        "\n",
        "# Remove original column (to avoid collision), then add encoded version\n",
        "X_demo_train_enc = (\n",
        "    X_demo_train_ohe.drop(columns=['colenroct99'], errors='ignore')\n",
        "    .join(colenr_enc_train)\n",
        ")\n",
        "\n",
        "# -----------------------------------------------\n",
        "# Step 4: Transform the TEST set using same encoder\n",
        "# -----------------------------------------------\n",
        "# Notice we use transform(), NOT fit_transform().\n",
        "# The mapping is already learned from train; we just apply it here.\n",
        "colenr_enc_test = pd.DataFrame(\n",
        "    oe.transform(X_demo_test_ohe[['colenroct99']]),  # transform only\n",
        "    columns=['colenroct99'],\n",
        "    index=X_demo_test_ohe.index\n",
        ")\n",
        "\n",
        "# Drop original, then add encoded version\n",
        "X_demo_test_enc = (\n",
        "    X_demo_test_ohe.drop(columns=['colenroct99'], errors='ignore')\n",
        "    .join(colenr_enc_test)\n",
        ")\n",
        "\n",
        "X_demo_train_enc.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76c81f08",
      "metadata": {
        "id": "76c81f08"
      },
      "source": [
        "## Encoding categorical features with medium or high cardinality\n",
        "\n",
        "When a categorical feature has **many unique values** (10, 50, or even thousands), creating a dummy variable (one-hot encoding) for each value quickly becomes impractical. Why?\n",
        "\n",
        "* **Too many columns**: one-hot encoding expands into hundreds of new features.\n",
        "* **Sparse data**: some categories may only appear a handful of times, giving the model little to learn from.\n",
        "* **Extreme case – IDs**: if each observation has a unique value (like student ID), one-hot encoding adds no useful information.\n",
        "\n",
        "\n",
        "#### Common Strategies\n",
        "\n",
        "1. **Top-K categories + “other”**\n",
        "\n",
        "   * Keep dummies only for the most common *k* categories.\n",
        "   * Group all rare categories into a single `\"other\"` column.\n",
        "   * Useful when a few categories dominate the data.\n",
        "\n",
        "2. **Feature hashing (hashing trick)**\n",
        "\n",
        "   * Map categories into a fixed number of bins using a hash function.\n",
        "   * You choose the number of bins (say 50), and all categories are compressed into them.\n",
        "   * Fast and memory-efficient, but collisions (different categories mapping to the same bin) can happen.\n",
        "\n",
        "#### More Advanced Options\n",
        "\n",
        "3. **Target / Mean Encoding**\n",
        "\n",
        "   * Replace each category with a summary statistic of the target variable (often the **mean target** for that category).\n",
        "   * Example: if the target is `defaulted (0/1)`, and category = `job_title`, we can encode each `job_title` with its default rate.\n",
        "   * Very powerful, especially with high cardinality, but must be done carefully:\n",
        "\n",
        "     * Can cause **data leakage** if computed on the whole dataset (solution: compute only on train data, or use cross-validation folds).\n",
        "     * Works best with regularization (smoothing rare categories toward the global mean).\n",
        "\n",
        "4. **Embeddings (deep learning approach)**\n",
        "\n",
        "   * Represent each category as a learned vector (like word embeddings in NLP).\n",
        "   * These vectors capture similarity among categories automatically.\n",
        "   * Typically used when features have very high cardinality (e.g., user IDs, product IDs) in neural networks.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "abf67dd9",
      "metadata": {
        "id": "abf67dd9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "region\n",
              "Eastern Europe     16\n",
              "East Asia          12\n",
              "Western Europe     12\n",
              "West Africa        11\n",
              "East Africa        10\n",
              "West Asia          10\n",
              "South Asia          7\n",
              "South America       7\n",
              "Southern Africa     7\n",
              "Central Africa      7\n",
              "Caribbean           6\n",
              "Oceania / Aus       6\n",
              "Central Asia        5\n",
              "North Africa        4\n",
              "North America       3\n",
              "Central America     3\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "covidtotals = pd.read_csv(\"covidtotals.csv\")\n",
        "feature_cols = ['location','population',\n",
        "    'aged_65_older','diabetes_prevalence','region']\n",
        "covidtotals = covidtotals[['total_cases'] + feature_cols].dropna()\n",
        "\n",
        "# Separate into train and test sets\n",
        "X_train, X_test, y_train, y_test =  \\\n",
        "  train_test_split(covidtotals[feature_cols],\\\n",
        "  covidtotals[['total_cases']], test_size=0.3, random_state=0)\n",
        "\n",
        "\n",
        "# use the one hot encoder for region\n",
        "X_train.region.value_counts()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca871f5f",
      "metadata": {
        "id": "ca871f5f"
      },
      "source": [
        "We can use the OneHotEncoder module from feature_engine again to\n",
        "encode the region feature. This time, we use the top_categories parameter to\n",
        "indicate that we only want to create dummies for the top six category values.\n",
        "Any values that do not fall into the top six will have a 0 for all of the dummies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "17f2b9ff",
      "metadata": {
        "id": "17f2b9ff"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>location</th>\n",
              "      <th>region_Eastern Europe</th>\n",
              "      <th>region_Western Europe</th>\n",
              "      <th>region_West Africa</th>\n",
              "      <th>region_East Asia</th>\n",
              "      <th>region_West Asia</th>\n",
              "      <th>region_East Africa</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>Israel</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>173</th>\n",
              "      <td>Senegal</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>Indonesia</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>187</th>\n",
              "      <td>Sri Lanka</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104</th>\n",
              "      <td>Kenya</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      location  region_Eastern Europe  region_Western Europe  \\\n",
              "97      Israel                      0                      0   \n",
              "173    Senegal                      0                      0   \n",
              "92   Indonesia                      0                      0   \n",
              "187  Sri Lanka                      0                      0   \n",
              "104      Kenya                      0                      0   \n",
              "\n",
              "     region_West Africa  region_East Asia  region_West Asia  \\\n",
              "97                    0                 0                 1   \n",
              "173                   1                 0                 0   \n",
              "92                    0                 1                 0   \n",
              "187                   0                 0                 0   \n",
              "104                   0                 0                 0   \n",
              "\n",
              "     region_East Africa  \n",
              "97                    0  \n",
              "173                   0  \n",
              "92                    0  \n",
              "187                   0  \n",
              "104                   1  "
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "ohe = OneHotEncoder(top_categories=6, variables=['region'])\n",
        "covidtotals_ohe = ohe.fit_transform(covidtotals)\n",
        "covidtotals_ohe.filter(regex='location|region',\n",
        "  axis=\"columns\").sample(5, random_state=99)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03931399",
      "metadata": {
        "id": "03931399"
      },
      "source": [
        "Feature hashing maps a large number of unique feature values to a smaller number of\n",
        "dummy variables. We can specify the number of dummy variables to create. However,\n",
        "collisions are possible; that is, some feature values might map to the same dummy variable\n",
        "combination. The number of collisions increases as we decrease the number of requested\n",
        "dummy variables.\n",
        "We can use HashingEncoder from category_encoders to do feature hashing.\n",
        "We use n_components to indicate that we want six dummy variables (we copy the\n",
        "region feature before we do the transform so that we can compare the original values to\n",
        "the new dummies):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "97bd2729",
      "metadata": {
        "id": "97bd2729"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>col_0</th>\n",
              "      <th>col_1</th>\n",
              "      <th>col_2</th>\n",
              "      <th>col_3</th>\n",
              "      <th>col_4</th>\n",
              "      <th>col_5</th>\n",
              "      <th>region2</th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>Caribbean</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>Central Africa</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>East Africa</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>North Africa</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Central America</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Eastern Europe</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>North America</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Oceania / Aus</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Southern Africa</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>West Asia</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Western Europe</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Central Asia</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>East Asia</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>South Asia</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>West Africa</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>South America</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    col_0  col_1  col_2  col_3  col_4  col_5          region2  count\n",
              "0       0      0      0      0      0      1        Caribbean      6\n",
              "1       0      0      0      0      0      1   Central Africa      7\n",
              "2       0      0      0      0      0      1      East Africa     10\n",
              "3       0      0      0      0      0      1     North Africa      4\n",
              "4       0      0      0      0      1      0  Central America      3\n",
              "5       0      0      0      0      1      0   Eastern Europe     16\n",
              "6       0      0      0      0      1      0    North America      3\n",
              "7       0      0      0      0      1      0    Oceania / Aus      6\n",
              "8       0      0      0      0      1      0  Southern Africa      7\n",
              "9       0      0      0      0      1      0        West Asia     10\n",
              "10      0      0      0      0      1      0   Western Europe     12\n",
              "11      0      0      0      1      0      0     Central Asia      5\n",
              "12      0      0      0      1      0      0        East Asia     12\n",
              "13      0      0      0      1      0      0       South Asia      7\n",
              "14      0      0      1      0      0      0      West Africa     11\n",
              "15      1      0      0      0      0      0    South America      7"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# use the hashing encoder for region\n",
        "X_train['region2'] = X_train.region\n",
        "he = HashingEncoder(cols=['region'], n_components=6)\n",
        "X_train_enc = he.fit_transform(X_train)\n",
        "X_train_enc.\\\n",
        " groupby(['col_0','col_1','col_2','col_3','col_4',\n",
        "   'col_5','region2']).\\\n",
        " size().reset_index().rename(columns={0:'count'})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "751523da",
      "metadata": {
        "id": "751523da"
      },
      "source": [
        "Unfortunately, this gives us a large number of collisions. For example, Caribbean, Central\n",
        "Africa, East Africa, and North Africa all get the same dummy variable values. In this case\n",
        "at least, using one-hot encoding and specifying the number of categories, as we did in the\n",
        "last section, was a better solution.\n",
        "\n",
        "\n",
        "#### Using mathematical transformations\n",
        " Sometimes, we want to use features that do not have a Gaussian distribution with\n",
        "a machine learning algorithm that assumes our features are distributed in that way. When\n",
        "that happens, we either need to change our minds about which algorithm to use (for\n",
        "example, we could choose KNN rather than linear regression) or transform our features so\n",
        "that they approximate a Gaussian distribution. This can be another use case for mean encoding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gJxG0rW4Qeg5",
      "metadata": {
        "id": "gJxG0rW4Qeg5"
      },
      "outputs": [],
      "source": [
        "# Read data\n",
        "covidtotals = pd.read_csv(\"covidtotals.csv\")\n",
        "feature_cols = ['location', 'population',\n",
        "                'aged_65_older', 'diabetes_prevalence', 'region']\n",
        "\n",
        "covidtotals = covidtotals[['total_cases'] + feature_cols].dropna()\n",
        "\n",
        "# Separate into train and test sets\n",
        "X = covidtotals[feature_cols]\n",
        "y = covidtotals['total_cases']      # use a Series (1D), not a DataFrame\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=0\n",
        ")\n",
        "\n",
        "# Categorical columns to target-encode\n",
        "cat_cols = ['region']\n",
        "\n",
        "# Set up the TargetEncoder (mean encoding using y)\n",
        "encoder = ce.TargetEncoder(cols=cat_cols)\n",
        "\n",
        "# Fit on training data ONLY (to avoid leakage), using y_train as target\n",
        "X_train_enc = encoder.fit_transform(X_train, y_train)\n",
        "\n",
        "# Transform test set using the encoder fitted on train\n",
        "X_test_enc = encoder.transform(X_test)\n",
        "\n",
        "# Now X_train_enc and X_test_enc are ready for modeling\n",
        "print(X_train_enc.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "r5YDeUmLSH3a",
      "metadata": {
        "id": "r5YDeUmLSH3a"
      },
      "source": [
        "### Advantages\n",
        "* **Handles high-cardinality categoricals**\n",
        "  Works better than one-hot when you have many unique categories (e.g. hundreds of locations), avoiding huge sparse matrices.\n",
        "* **Keeps feature space small**\n",
        "  Each categorical variable becomes **one numeric column**, not dozens/hundreds.\n",
        "* **Often boosts model performance**\n",
        "  Especially for tree-based models (Random Forest, XGBoost, LightGBM) that like meaningful numeric encodings.\n",
        "* **Captures signal from the target**\n",
        "  If some categories are strongly associated with high/low target values, mean encoding makes that explicit.\n",
        "\n",
        "### Disadvantages / Risks\n",
        "* **Target leakage**\n",
        "  If you compute the mean using the *whole dataset* (train + test), you “peek” at the test labels → overly optimistic performance.\n",
        "* **Overfitting on rare categories**\n",
        "  Categories with very few examples can get extreme means that don’t generalize.\n",
        "* **Less interpretable**\n",
        "  “Region = 1234.5” is less intuitive than “Region = Europe / Asia / …”.\n",
        "* **Needs careful implementation**\n",
        "  Must be fit only on training data; often with smoothing and/or cross-validation schemes.\n",
        "### When to use\n",
        "Use target/mean encoding when:\n",
        "* You have **categorical variables**, especially with **many levels**.\n",
        "* You’re training **supervised models** (regression or classification).\n",
        "* You’re comfortable handling **data leakage** properly:\n",
        "  * Fit encoder on **train only** (like you did with `TargetEncoder` and `y_train`).\n",
        "  * Optionally use cross-validation-based target encoding in more advanced setups.\n",
        "\n",
        "Avoid or be cautious when:\n",
        "* Dataset is **small** and categories are many and very rare.\n",
        "* You’re doing quick “intro” ML and want **maximum interpretability** → one-hot may be safer and simpler to explain.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jMuYx15fRuMZ",
      "metadata": {
        "id": "jMuYx15fRuMZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
