{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4reF3LGsUX87"
   },
   "source": [
    "# Learning theory - continued\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "itZSqNkPUX89"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score, auc, mean_squared_error, accuracy_score, precision_score, recall_score, f1_score, ConfusionMatrixDisplay, RocCurveDisplay\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oXM3VrJAUX9I"
   },
   "source": [
    "## Let's try to put it all into practice\n",
    "\n",
    "We will use a german credit reisk dataset that classifies people into good or bad risks from:\n",
    "https://archive.ics.uci.edu/dataset/144/statlog+german+credit+data\n",
    "\n",
    "First we will need to load our data (you will also need to upload the data to colab first):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ewREl2W9UX9I"
   },
   "outputs": [],
   "source": [
    "# Load the CSV file into a DataFrame\n",
    "data = pd.read_csv('4_german_credit_data.csv')\n",
    "\n",
    "# Display the first few rows to get an initial understanding\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6IPIW6RDUX9I"
   },
   "source": [
    "After loading the dataset, it's essential to get a quick overview of its characteristics, such as the number of rows and columns, the data types of the columns, and basic statistics of numerical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fO2IjQoBUX9I"
   },
   "outputs": [],
   "source": [
    "# Get a concise summary of the dataset\n",
    "data.info()\n",
    "\n",
    "# Get basic statistics of numerical columns\n",
    "data.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D7bfkC-zUX9I"
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ykNVFff6UX9I"
   },
   "source": [
    "What we can see is that the dataset contains 21 columns: 20 features and 1 target column (CreditRisk).\n",
    "Features are a mix of categorical (e.g., CheckingAccountStatus, CreditHistory) and numerical (e.g., Duration, CreditAmount) data types.\n",
    "The target column CreditRisk has two classes: 1 (Good Risk) and 2 (Bad Risk).\n",
    "\n",
    "Note we have lot's of categorical data, we would need to understand how these features work and how the values are distributed. For example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KaEF_xf9UX9I"
   },
   "outputs": [],
   "source": [
    "# Check the distribution of some categorical features\n",
    "plt.figure(figsize=(10, 6))\n",
    "data['CheckingAccountStatus'].value_counts().plot(kind='bar')\n",
    "plt.title('Distribution of Checking Account Status')\n",
    "plt.xlabel('Status')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QY9-poYMUX9I"
   },
   "outputs": [],
   "source": [
    "# Plot the CheckingAccountStatus against the CreditRisk\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=data, x='CheckingAccountStatus', hue='CreditRisk')\n",
    "plt.title('Distribution of CreditRisk by CheckingAccountStatus')\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Checking Account Status')\n",
    "plt.legend(title='Credit Risk', loc='upper right')\n",
    "plt.grid(axis='y')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eoZeC_x5UX9I"
   },
   "source": [
    "1. A11 (< 0 DM): This category has the highest proportion of high risk.\n",
    "2. A12 (0 <= ... < 200 DM): The number of low-risk individuals is slightly higher than high-risk individuals.\n",
    "3. A13 (>= 200 DM / salary assignments for at least 1 year): A higher proportion of individuals are low risk, but there is still a significant number of high-risk individuals.\n",
    "4. A14 (no checking account): A majority of individuals in this category are labeled as low risk.\n",
    "\n",
    "We hsould really look at all the variables.\n",
    "\n",
    "Numerical values are a bit easier to look at:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4JlLXApRUX9J"
   },
   "outputs": [],
   "source": [
    "# Calculate the correlation of numerical features with the target variable 'CreditRisk'\n",
    "correlations = data.corr(numeric_only=True)['CreditRisk'].sort_values()\n",
    "\n",
    "correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ixUDeLptUX9J"
   },
   "source": [
    "Let's pick some features:\n",
    "\n",
    "* Duration: As we observed, it has a positive correlation with the target variable.\n",
    "* Amount: The amount of the loan can be an indicator of risk.\n",
    "* Age: Age might play a role in determining creditworthiness.\n",
    "* CheckingAccountStatus: As seen in the plot, different levels of checking account status have different risk distributions.\n",
    "* CreditHistory: The credit history of a person can be a significant indicator of their credit risk.\n",
    "* Purpose: The purpose of the loan can influence the risk associated with it.\n",
    "* SavingsAccountBonds: Savings can be an indicator of financial stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E4GfTY5gUX9J"
   },
   "outputs": [],
   "source": [
    "# Selecting initial set of features\n",
    "selected_features = ['Duration', 'CreditAmount', 'Age', 'CheckingAccountStatus',\n",
    "                     'CreditHistory', 'Purpose', 'SavingsAccountBonds']\n",
    "\n",
    "# Plotting correlation matrix for these features and the target variable\n",
    "correlation_matrix = data[selected_features + ['CreditRisk']].corr(numeric_only=True)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Correlation Matrix of Selected Features and Target Variable')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aUR9eL1dUX9J"
   },
   "source": [
    "We still need to think about cleaning and transforming our data a little:\n",
    "\n",
    "Handle Missing Data: Ensure that there's no missing data in the dataset or decide on a strategy to handle them (e.g., imputation).\n",
    "\n",
    "Convert Categorical Data: Convert categorical variables into a format suitable for KNN, typically using one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qc3o49ezUX9J"
   },
   "outputs": [],
   "source": [
    "# Check for missing values in the dataset\n",
    "missing_values = data.isnull().sum()\n",
    "print(f\"missing values - {missing_values}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zj5F_9XTwiEl"
   },
   "outputs": [],
   "source": [
    "# Drop rows with missing values if any exist\n",
    "data_cleaned = data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v1m6OF7aUX9J"
   },
   "source": [
    "We are going to apply one-hot encoding to the categorical features.\n",
    "\n",
    "One-Hot Encoding is a method used to convert categorical data variables so they can be provided to machine learning algorithms to improve predictions. Machine learning algorithms cannot work with categorical data directly. Categorical data must be converted to numbers. One of the most common ways to do this transformation is by using one-hot encoding.\n",
    "\n",
    "How It Works:\n",
    "Identify Unique Categories: For each categorical variable, identify the unique categories it can take.\n",
    "\n",
    "Create New Columns: For each unique category, a new binary (0 or 1) column is created.\n",
    "\n",
    "Binary Indication:\n",
    "\n",
    "For each record, the column corresponding to the category the variable takes value will have a '1', and all other new columns will have a '0'.\n",
    "This means that out of all the new columns for a categorical variable, only one can take the value '1' for a given record.\n",
    "\n",
    "**Important** One drawback of one-hot encoding is that it can increase the dimensionality of the dataset significantly if the categorical variable has many unique values.\n",
    "It can also lead to multicollinearity, a situation where two or more variables are highly correlated. In the context of one-hot encoding, one variable can be predicted from the others. This is called the dummy variable trap.\n",
    "To avoid the dummy variable trap, one common practice is to drop one of the one-hot encoded columns (hence, for a variable with n categories, we keep nâˆ’1 dummy columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "isSvyW46UX9J"
   },
   "outputs": [],
   "source": [
    "# Identify numerical and categorical columns\n",
    "numeric_features = ['Duration', 'CreditAmount', 'Age']\n",
    "categorical_features = ['CheckingAccountStatus', 'CreditHistory', 'Purpose', 'SavingsAccountBonds']\n",
    "\n",
    "# Apply one-hot encoding using pandas get_dummies method\n",
    "data_encoded = pd.get_dummies(data_cleaned, columns=categorical_features)\n",
    "\n",
    "# Display the first few rows of the encoded data\n",
    "data_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "44ULv4c5UX9K"
   },
   "outputs": [],
   "source": [
    "data_encoded.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XklQzfW9UX9K"
   },
   "outputs": [],
   "source": [
    "encoded_features = data_encoded.columns[17:].to_list() + numeric_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RT0nMIpI2Yye"
   },
   "outputs": [],
   "source": [
    "encoded_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rasm63dGUX9K"
   },
   "source": [
    "Let's create our data finally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bsbFPrykUX9K"
   },
   "outputs": [],
   "source": [
    "X = data_encoded[encoded_features]\n",
    "y = data_encoded['CreditRisk'] - 1  # Convert to 0 and 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mFargi4dUX9K"
   },
   "source": [
    "Now remember, before we go any further we need to split our data. This is essential to assesing our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BL-eFGWkUX9K"
   },
   "outputs": [],
   "source": [
    "# Split your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n5r0Z43fUX9K"
   },
   "outputs": [],
   "source": [
    "# Check the shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PDXScJ4DUX9K"
   },
   "source": [
    "Now we can initialise our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a_79TjgPUX9K"
   },
   "outputs": [],
   "source": [
    "# Initialise a KNeighborsClassifier with n_neighbors=5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AatHS5_FUX9K"
   },
   "source": [
    "Next we fit our model to the data, be careful which data to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gkZOddg5UX9L"
   },
   "outputs": [],
   "source": [
    "# Fit the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-LOQ9RD7UX9L"
   },
   "source": [
    "Next we apply our model to make prediction, in this case on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gcZLbnVgw8St"
   },
   "outputs": [],
   "source": [
    "# generate predictions on the test test\n",
    "y_pred ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at what's happening, it's always worth checking your confusion matrix.\n",
    "\n",
    "```confusion_matrix(y_true, y_pred)```\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Display the confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "irb2FT2C1XP_"
   },
   "source": [
    "Complete the below function to calculate our metrics. You can make use of confusion matrix:\n",
    "\n",
    "![CM](https://miro.medium.com/v2/resize:fit:4800/format:webp/1*uR09zTlPgIj5PvMYJZScVg.png)\n",
    "\n",
    "```\n",
    "tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U8ZQEstN1XP_"
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(y_true, y_pred):\n",
    "    \"\"\"Compute accuracy. What propotion of predictions were correct.\"\"\"\n",
    "\n",
    "def compute_recall(y_true, y_pred):\n",
    "    \"\"\"Compute recall (sensitivity). How all actual positive values, how many did we get.\"\"\"\n",
    "\n",
    "def compute_precision(y_true, y_pred):\n",
    "    \"\"\"Compute precision. Of all positive predictions, how many were correct.\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-eQZ5aP81XP_"
   },
   "source": [
    "Apply our new functions to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j96_eqHR1XP_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another metric of interest is the F1 score, this help us by balancing precision and recall.\n",
    "\n",
    "$$\n",
    "F_1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "Create a function to calculate f1 score and apply it to our model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_f1(y_true, y_pred):\n",
    "    \"\"\"Compute f1.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c8FQNJhd3NrU"
   },
   "source": [
    "Remember we have the ability to adjust the focus of our classifier. The ROC curve let's us explore our options in this adjustment and let's us compare classifiers for their ability to give us more options. Let's apply\n",
    "```\n",
    "fpr, tpr, thresholds = roc_curve(y_true, y_score)\n",
    "```\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MacUAEox3MxO"
   },
   "outputs": [],
   "source": [
    "# first we need the class probability estimates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JjpbaMZN-id8"
   },
   "outputs": [],
   "source": [
    "# then we can use the roc_curve function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yFV4i9vB8v7C"
   },
   "source": [
    "We also also interested in the area under the curve metric. We can use:\n",
    "\n",
    "```\n",
    "roc_auc_score(y_true, y_score)\n",
    "```\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "14G4_Xiq9AUq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LbGXN-wI9df4"
   },
   "source": [
    "We can also calculate based on fpr, and tpr with `auc(fpr, tpr)`\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.auc.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xKpqbDL59heb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fPAbkR0_7x-Z"
   },
   "source": [
    "Typically we like to visualise this curve, we can use the values we generated or the function:\n",
    "```\n",
    "RocCurveDisplay(fpr= , tpr= , roc_auc= ).plot()\n",
    "```\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.RocCurveDisplay.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xj0G4jAP4tyC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ebwsO6oHUX9L"
   },
   "source": [
    "Not a great model! Still we have some options to adjust, imagine we are very concerned with False negatives, credit risks we missed. How can we adjust our classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VMoErFxeUX9L"
   },
   "outputs": [],
   "source": [
    "# Predict class probabilities using predict_proba(X_test)[:, 1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UkYSGdxb3HG_"
   },
   "outputs": [],
   "source": [
    "# Adjust the threshold to decrease the FNR (and increase TPR)\n",
    "threshold =\n",
    "y_pred_adjusted = (y_prob > threshold).astype(int)\n",
    "\n",
    "# Calculate and print metrics with the adjusted threshold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SX6eq-B2WW0h"
   },
   "source": [
    "## Exercises\n",
    "\n",
    "Ok, time for you to give it all a try. We have already created all the data you need, split it into test and training datasets. Your job:\n",
    "\n",
    "First, fit a linear model (LinearRegression) and different KNN models with different values of k to the data see what you think perfodms best. Remember the steps:\n",
    "\n",
    "* Initialise the model\n",
    "\n",
    "* Fit the model\n",
    "\n",
    "* Make predictions (a little trickier for the linearRegression, but you have lots of examples).\n",
    "\n",
    "* Evaluate the model by evaluating thepredictions\n",
    "\n",
    "Compare the models and decide the model that works best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LryO7iQ8UX9L"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fMlEkWIMXoVa"
   },
   "source": [
    "Using the model you believe works best, adjust the threshold to:\n",
    "\n",
    "\n",
    "*   Maximise precision\n",
    "*   Maximise recall\n",
    "*   Maximise accuracy\n",
    "\n",
    "Taking a look at the ROC might help.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RE4bI3lNX3za"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pZJ32yWdYIfJ"
   },
   "source": [
    "Try to improve your model by using more (or less) information we have from the dataset. That is change the features we are including.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d49oAseuYSTb"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
