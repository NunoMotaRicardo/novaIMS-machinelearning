{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHnbAWRbag0C"
      },
      "source": [
        "# Exploring Unsupervised Learning Techniques\n",
        "\n",
        "Here we will explore Unsupervised Learning, a subfield of machine learning where the goal is to model the underlying structure or distribution in the data in order to learn more about it. Unlike supervised learning, unsupervised learning does not involve labeled outcomes, making it well-suited for exploratory data analysis, clustering, dimensionality reduction, and anomaly detection, among other applications.\n",
        "\n",
        "We will look at:\n",
        "* Clustering Algorithms: Understand and implement popular clustering algorithms like k-means, hierarchical clustering, and DBSCAN.\n",
        "* Dimensionality Reduction: Delve into techniques such as Principal Component Analysis (PCA) and t-Distributed Stochastic Neighbor Embedding (t-SNE).\n",
        "\n",
        "Let's load our libraries and data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "yellowbrick is not installed. Installing now...\n",
            "yellowbrick has been successfully installed.\n"
          ]
        }
      ],
      "source": [
        "# first we will check if you have yellowbrick and if not install\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# Function to check if a package is installed and install it if necessary\n",
        "def check_and_install(package_name):\n",
        "    try:\n",
        "        # Try to import the package\n",
        "        __import__(package_name)\n",
        "        print(f\"{package_name} is already installed.\")\n",
        "    except ImportError:\n",
        "        # If not installed, install the package\n",
        "        print(f\"{package_name} is not installed. Installing now...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_name])\n",
        "        print(f\"{package_name} has been successfully installed.\")\n",
        "\n",
        "# Check and install 'yellowbrick'\n",
        "check_and_install(\"yellowbrick\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "z0-UY-GZZnyK"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'distutils'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LabelEncoder, StandardScaler, OneHotEncoder\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01myellowbrick\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcluster\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KElbowVisualizer, SilhouetteVisualizer\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcluster\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KMeans\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m silhouette_score\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\my-git\\DataScience-novaIMS\\MachineLearning\\.venv\\Lib\\site-packages\\yellowbrick\\__init__.py:31\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_version, __version_info__\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Import the style management functions\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstyle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrcmod\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m reset_defaults, reset_orig\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstyle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrcmod\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m set_aesthetic, set_style, set_palette\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstyle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpalettes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m color_palette, set_color_codes\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\my-git\\DataScience-novaIMS\\MachineLearning\\.venv\\Lib\\site-packages\\yellowbrick\\style\\__init__.py:20\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[33;03mManage the style and aesthetic of the yellowbrick library.\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m##########################################################################\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m## Imports\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m##########################################################################\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpalettes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrcmod\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\my-git\\DataScience-novaIMS\\MachineLearning\\.venv\\Lib\\site-packages\\yellowbrick\\style\\colors.py:33\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01myellowbrick\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m YellowbrickValueError\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Check to see if matplotlib is at least sorta up to date\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdistutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LooseVersion\n\u001b[32m     35\u001b[39m mpl_ge_150 = LooseVersion(mpl.__version__) >= \u001b[33m\"\u001b[39m\u001b[33m1.5.0\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m##########################################################################\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m## Color Utilities\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m##########################################################################\u001b[39;00m\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'distutils'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
        "from yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "from mpl_toolkits.mplot3d import Axes3D\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#mount my google drive (from vscode)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnBGHj6fZZGk"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('9_marketing_campaign_cleaned.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYa169ygZbD5"
      },
      "outputs": [],
      "source": [
        "print(\"Number of datapoints:\", len(data))\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "phyvIBwtNOJw"
      },
      "outputs": [],
      "source": [
        "# Check for NA's with isna and sum\n",
        "data.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJKzfVvMaPYB"
      },
      "source": [
        "For more info about dataset:\n",
        "\n",
        "https://www.kaggle.com/datasets/imakash3011/customer-personality-analysis\n",
        "\n",
        "Let's have a little look at the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGg9iU1sd34a"
      },
      "outputs": [],
      "source": [
        "#Plotting following features\n",
        "To_Plot = [ \"Income\", \"Recency\", \"Customer_For\", \"Age\", \"Spent\", \"Is_Parent\"]\n",
        "plt.figure()\n",
        "sns.pairplot(data[To_Plot], hue= \"Is_Parent\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBHzLyrNagJ3"
      },
      "source": [
        "What about categorical variables? We will need to deal with these. Remember we can find our columns by looking at dtype:\n",
        "\n",
        "```\n",
        "select_dtypes(include='int64').columns.tolist()\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwiB7E-WLO2y"
      },
      "outputs": [],
      "source": [
        "# use .info to check data types\n",
        "data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKqtD1KcfAgL"
      },
      "outputs": [],
      "source": [
        "# find categorical columns\n",
        "categorical_columns = "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# check value counts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yL5H5JweLb17"
      },
      "source": [
        "Let's onehot encode for now, even though this isn't perfect for clustering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pEhMoc_bfHYr"
      },
      "outputs": [],
      "source": [
        "encoder = OneHotEncoder(sparse_output=False, drop=None)\n",
        "\n",
        "encoded = encoder.fit_transform(data[categorical_columns])\n",
        "\n",
        "# Create a DataFrame for the encoded data\n",
        "encoded_df = pd.DataFrame(encoded, columns=encoder.get_feature_names_out(categorical_columns), index=data.index)\n",
        "\n",
        "# Drop the original categorical columns and concatenate the new one-hot encoded columns\n",
        "data = data.drop(columns=categorical_columns).join(encoded_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PAxoSUkfd4y"
      },
      "source": [
        "All features are now numerical and we can run them in k means. Think about whether this makes sense in practice.\n",
        "\n",
        "However, remember scaling is important for clustering. We will apply StandardScalar which we have seen before.\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejK8Kew8fZOL"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "scaler.fit(data)\n",
        "scaled_data = pd.DataFrame(scaler.transform(data),columns= data.columns )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# check our results:\n",
        "scaled_data.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpPx1wlyj9F8"
      },
      "source": [
        "Outliers can be a big problem! Let's remove by removing any values greater than 3 standard deviaitons from the mean.\n",
        "\n",
        "Filter thedata appropriately and drop where we find NA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6uimyQhGkC7M"
      },
      "outputs": [],
      "source": [
        "scaled_data = scaled_data[(scaled_data > -3) & (scaled_data < 3)].dropna()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jC_w0jU_gGBe"
      },
      "source": [
        "### K Means\n",
        "\n",
        "Great our data is all ready, let's start to look at our clustering approaches. We follow a very familiar approach, only we don't need to split our data into training and test splits, we just:\n",
        "\n",
        "* Model Initialization - Initialize the `KMeans` model with desired parameters. The number of clusters `n_clusters` (k) is a required input.\n",
        "* Model Fitting - Fit the model to your data using `.fit(X)`.\n",
        "\n",
        "https://scikit-learn.org/1.5/modules/generated/sklearn.cluster.KMeans.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZGqbOauf0XT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yr1msDag59z"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lifOsVcghX2L"
      },
      "source": [
        "We can then look at the clusters designated by checking the labels `.labels_` and the centroids of the clusters `.cluster_centers_`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NA7y6VOuhSZL"
      },
      "outputs": [],
      "source": [
        "labels = \n",
        "labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8-6p5jche6L"
      },
      "outputs": [],
      "source": [
        "centroids = \n",
        "centroids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Its usuall easier to understand if we plot our centeroids."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "centroid_df = pd.DataFrame(centroids, columns=scaled_data.columns)\n",
        "\n",
        "# Plot centroids\n",
        "plt.figure(figsize=(10, 6))\n",
        "centroid_df.T.plot(kind='bar', figsize=(10, 6), legend=True)\n",
        "plt.title('Centroids for Each Cluster')\n",
        "plt.ylabel('Centroid Value')\n",
        "plt.xlabel('Features')\n",
        "plt.legend(title='Cluster')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCJfFuCTiscV"
      },
      "source": [
        "Let's try to visualise, here is some example code where we set the hue of the data based on our finding of the labels:\n",
        "\n",
        "```\n",
        "pl = sns.scatterplot(data = scaled_data,x=\"Spent\", y=\"Income\",hue=labels)\n",
        "pl.set_title(\"Cluster's Profile Based On Income And Spending\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4uEIOxzoA9M"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILft9O-tIlfz"
      },
      "source": [
        "Here we provide a function to look at 3 features at a time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0n4SZ-0aiu5e"
      },
      "outputs": [],
      "source": [
        "def plot_3d_clusters(data, labels, feature_names, colors=None, markers=None):\n",
        "    \"\"\"\n",
        "    Plots a 3D scatter plot for clustering data with up to 10 different clusters.\n",
        "    \"\"\"\n",
        "    if len(feature_names) != 3:\n",
        "        raise ValueError(\"Three feature names must be provided for x, y, and z axes.\")\n",
        "\n",
        "    # Default color and marker arrays for up to 10 clusters\n",
        "    default_colors = ['r', 'g', 'b', 'c', 'm', 'y', 'k', 'orange', 'purple', 'brown']\n",
        "    default_markers = ['o', '^', 's', 'P', '*', 'D', 'x', '+', 'H', '1']\n",
        "\n",
        "    # Use default colors and markers if none are provided\n",
        "    if colors is None:\n",
        "        colors = default_colors\n",
        "    if markers is None:\n",
        "        markers = default_markers\n",
        "\n",
        "    # Check if there are enough colors and markers for the number of clusters\n",
        "    unique_labels = np.unique(labels)\n",
        "    if len(unique_labels) > len(colors) or len(unique_labels) > len(markers):\n",
        "        raise ValueError(\"Not enough colors or markers for the number of clusters.\")\n",
        "\n",
        "    fig = plt.figure(figsize=(10, 8))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "    for label, color, marker in zip(unique_labels, colors, markers):\n",
        "        idx = (labels == label)\n",
        "        ax.scatter(data.loc[idx, feature_names[0]], data.loc[idx, feature_names[1]], data.loc[idx, feature_names[2]], label=f'Cluster {label}', c=color, marker=marker)\n",
        "\n",
        "    ax.set_xlabel(feature_names[0])\n",
        "    ax.set_ylabel(feature_names[1])\n",
        "    ax.set_zlabel(feature_names[2])\n",
        "    ax.legend()\n",
        "    ax.set_title('3D Cluster Plot with Distinct Colors and Markers')\n",
        "\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-npLZFzlNh_I"
      },
      "source": [
        "See if you can use the function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KoMyiMM8Jhb9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhaZnQXWmhgE"
      },
      "source": [
        "#### Choosing k\n",
        "\n",
        "Ok but what value of k should we use? We can Use an elbow visualiser to help us pick.\n",
        "\n",
        "https://www.scikit-yb.org/en/latest/api/cluster/elbow.html\n",
        "\n",
        "We need to specify our classifier and max k `KElbowVisualizer(alg, k=)` then we can use fit and show.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9rr5mTyjr1s"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEmZHDlinY-D"
      },
      "source": [
        "Or we can look at silhoute score. We will need a loop to calculate the score at each point in time and add to a list. Try setting the k means init to 'k-means++'.\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBh1AvjSnYKN"
      },
      "outputs": [],
      "source": [
        "# List to store the silhouette scores\n",
        "silhouette_scores = []\n",
        "\n",
        "# Range of k values to try\n",
        "k_range = range(2, 11)  # Usually, a minimum of 2 clusters is considered\n",
        "# Compute silhouette scores for different k values\n",
        "for k in k_range:\n",
        "    kmeans = KMeans(n_clusters=k, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
        "    kmeans.fit(scaled_data)  # Exclude the 'Cluster_Labels' column for fitting\n",
        "    labels = kmeans.labels_\n",
        "    score = silhouette_score(scaled_data, labels)\n",
        "    silhouette_scores.append(score)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLeNE0Y4kp5k"
      },
      "outputs": [],
      "source": [
        "# Plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(k_range, silhouette_scores, marker='o')\n",
        "plt.xlabel('Number of clusters (k)')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.title('Choosing Optimal k using Silhouette Score')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Find the optimal k (where silhouette score is maximum)\n",
        "optimal_k = k_range[silhouette_scores.index(max(silhouette_scores))]\n",
        "print(f\"Optimal K: {optimal_k} with score {max(silhouette_scores)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also look at the clusters themselves with the slihoute visualiser:\n",
        "\n",
        "https://www.scikit-yb.org/en/latest/api/cluster/silhouette.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvgOI9c8rdv9"
      },
      "source": [
        "## Hierarchical Methods:\n",
        "Let's examine how agglomerative and divisive techniques build the dendrogram in hierarchical clustering."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49YBtDEkveFW"
      },
      "source": [
        "We need to set the linkage type we will use by creating a linkage object, here we use ward.\n",
        "\n",
        "https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html\n",
        "\n",
        "```\n",
        "Z = linkage(hc_data, method='ward')\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6YJ0UK_mvkem"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Z"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-mc8CFFvniW"
      },
      "source": [
        "With our linkage type we can create a dendrogram.\n",
        "\n",
        "https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.dendrogram.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4l27IxLn1R1"
      },
      "outputs": [],
      "source": [
        "# Create a dendrogram\n",
        "plt.figure(figsize=(12, 6))\n",
        "dendrogram(Z)\n",
        "plt.title('Hierarchical Clustering Dendrogram (Ward)')\n",
        "plt.xlabel('Sample index')\n",
        "plt.ylabel('Euclidean distance')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IWSFgdPvxPW"
      },
      "source": [
        "Next we can fit our all an `AgglomerativeClustering` model. We follow the same steps: initialise, fit, predict (on our case just check labels with `.labels_`)\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html\n",
        "\n",
        "To initialise we need to set `n_clusters`, `metric` for which we cna use 'euclidean' and `linkage` for which again we can use'ward'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CifTdBtdq3cl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUGcbTadwb5B"
      },
      "source": [
        "Let's plot our clusters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MTB1cRoEpNxd"
      },
      "outputs": [],
      "source": [
        "labels = \n",
        "pl = sns.scatterplot(data = scaled_data,x=\"Spent\", y=\"Income\",hue=labels)\n",
        "pl.set_title(\"Cluster's Profile Based On Income And Spending\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqVD2tAeweQ0"
      },
      "source": [
        "Again we can explore values for k using KElbowVisualiser and Silhoute score:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBF6h63LphYk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgQuRfbZqHVN"
      },
      "outputs": [],
      "source": [
        "# List to store the silhouette scores\n",
        "silhouette_scores = []\n",
        "\n",
        "# Range of k values to try\n",
        "k_range = range(2, 11)  # Usually, a minimum of 2 clusters is considered\n",
        "\n",
        "# Compute silhouette scores for different k values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pInySZIgq_0n"
      },
      "outputs": [],
      "source": [
        "# Plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(k_range, silhouette_scores, marker='o')\n",
        "plt.xlabel('Number of clusters (k)')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.title('Choosing Optimal k using Silhouette Score')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Find the optimal k (where silhouette score is maximum)\n",
        "optimal_k = k_range[silhouette_scores.index(max(silhouette_scores))]\n",
        "optimal_k, max(silhouette_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MemRuq7MrvNl"
      },
      "source": [
        "### DBSCAN\n",
        "Finally, let's take a look at DBSCAN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izoOGU2ywnqw"
      },
      "source": [
        "We initialise `DBSCAN` with `eps` and `min_samples`\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7Sm_zRowqcY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCm1Gz11w5lg"
      },
      "source": [
        "DBSCAN will identify the number of clusters iteself, let's take a look at what it found:\n",
        "\n",
        "```\n",
        "u, c = np.unique(X, return_counts=True)\n",
        "\n",
        "cluster_summary_df = pd.DataFrame({\n",
        "    'Cluster_ID': u,\n",
        "    'Number_of_Points': c\n",
        "})\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_K5IQNpsr7S"
      },
      "outputs": [],
      "source": [
        "dbscan_labels = \n",
        "\n",
        "# Count the number of unique clusters and noise points\n",
        "unique_clusters, counts = np.unique(dbscan_labels, return_counts=True)\n",
        "\n",
        "# Number of unique clusters (excluding noise)\n",
        "num_clusters = len(unique_clusters) - (1 if -1 in unique_clusters else 0)\n",
        "\n",
        "\n",
        "# Create a DataFrame to display the cluster IDs and their corresponding counts\n",
        "cluster_summary_df = pd.DataFrame({\n",
        "    'Cluster_ID': unique_clusters,\n",
        "    'Number_of_Points': counts\n",
        "})\n",
        "\n",
        "# Highlight the noise points (-1)\n",
        "cluster_summary_df['Type'] = ['Noise' if id == -1 else 'Cluster' for id in unique_clusters]\n",
        "\n",
        "cluster_summary_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5eP2rgLw_5W"
      },
      "source": [
        "Let's see our plot again:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1LNR_mNsQku"
      },
      "outputs": [],
      "source": [
        "pl = sns.scatterplot(data = db_data,x=db_data[\"Spent\"], y=db_data[\"Income\"],hue=dbscan_labels)\n",
        "pl.set_title(\"Cluster's Profile Based On Income And Spending\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fP-geLVExCrW"
      },
      "source": [
        "Maybe it would be helpful to work with less data, try again with a subset of the features:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O03hcPxQses2"
      },
      "outputs": [],
      "source": [
        "db_data_small = scaled_data[[\"Spent\", \"Income\", \"Age\"]].copy()\n",
        "dbscan_small = DBSCAN(eps=0.15, min_samples=5)\n",
        "dbscan_labels_small = dbscan_small.fit_predict(db_data_small)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Count the number of unique clusters and noise points\n",
        "unique_clusters, counts = np.unique(dbscan_labels_small, return_counts=True)\n",
        "\n",
        "# Number of unique clusters (excluding noise)\n",
        "num_clusters = len(unique_clusters) - (1 if -1 in unique_clusters else 0)\n",
        "\n",
        "\n",
        "# Create a DataFrame to display the cluster IDs and their corresponding counts\n",
        "cluster_summary_df = pd.DataFrame({\n",
        "    'Cluster_ID': unique_clusters,\n",
        "    'Number_of_Points': counts\n",
        "})\n",
        "\n",
        "# Highlight the noise points (-1)\n",
        "cluster_summary_df['Type'] = ['Noise' if id == -1 else 'Cluster' for id in unique_clusters]\n",
        "\n",
        "cluster_summary_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxlvYKaAQeeY"
      },
      "source": [
        "### Exercise\n",
        "\n",
        "We have seen how to apply different unsupervised techniques to clustering. But we have not produced very useful output and there are a number of decisions that may result in not very effective clustering. Try to imporve the clustering:\n",
        "* Look at the features, in particular their correlations, and check we are not adding too much importance to one type of feature by including it multiple times.\n",
        "* Think about the data types of features, have we dealt with these well? What other approaches could we take?\n",
        "* Think about the difference between centroids and medoids? Which is more useful? can you produce both?\n",
        "\n",
        "Make use of additional visualisation functionality from yellowbricks:\n",
        "\n",
        "https://www.scikit-yb.org/en/latest/api/cluster/icdm.html\n",
        "https://www.scikit-yb.org/en/latest/api/cluster/silhouette.html\n",
        "\n",
        "Try to produce the most useful clustering of the customer data for a sales or marketing team. Understand the difference between the clusters.\n",
        "* One you have found the \"best\" clustering give each cluster a name and describe it's proporties."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZaAFNdoItXJ-"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
